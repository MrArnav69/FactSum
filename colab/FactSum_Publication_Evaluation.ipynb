{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FactSum: Publication-Ready Evaluation\n",
    "\n",
    "This notebook runs **complete factuality evaluation** for top-tier journal submission.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ **500+ samples** (configurable)\n",
    "- ‚úÖ **8 factuality metrics** (including QAFactEval)\n",
    "- ‚úÖ **3 baseline models** (PEGASUS, PRIMERA, Hierarchical)\n",
    "- ‚úÖ **GPU acceleration**\n",
    "- ‚úÖ **Results saved to Google Drive**\n",
    "\n",
    "**Runtime**: ~2-4 hours depending on sample size and GPU availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets evaluate rouge-score bert-score sentence-transformers\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q nltk pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install QAFactEval (ONLY WORKS ON LINUX x86)\n",
    "!pip install qafacteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/FactSum_Results'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURATION =====\n",
    "# Adjust these for your needs\n",
    "\n",
    "NUM_SAMPLES = 500  # Recommended: 500 for short papers, 1000+ for main venues\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Models to evaluate\n",
    "MODELS = {\n",
    "    'PEGASUS': 'google/pegasus-multi_news',\n",
    "    'PRIMERA': 'allenai/PRIMERA',\n",
    "    # Add your hierarchical model here if you upload it\n",
    "}\n",
    "\n",
    "# Metrics to use\n",
    "USE_QAFACTEVAL = True  # Original Salesforce implementation\n",
    "USE_SUMMAC = True\n",
    "USE_FACTCC = True\n",
    "USE_BARTSCORE = True\n",
    "USE_BERTSCORE = True\n",
    "USE_UNIEVAL = True\n",
    "USE_ALIGNSCORE = True\n",
    "USE_QAGS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Multi-News dataset...\")\n",
    "dataset = load_dataset(\"multi_news\", split=\"test\")\n",
    "print(f\"Full test set size: {len(dataset)}\")\n",
    "\n",
    "# Sample\n",
    "np.random.seed(RANDOM_SEED)\n",
    "indices = np.random.choice(len(dataset), min(NUM_SAMPLES, len(dataset)), replace=False)\n",
    "samples = [dataset[int(i)] for i in indices]\n",
    "print(f\"Selected {len(samples)} samples for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 QAFactEval (Original Salesforce)\n",
    "if USE_QAFACTEVAL:\n",
    "    print(\"Loading QAFactEval (Original)...\")\n",
    "    from qafacteval import QAFactEval\n",
    "    qafacteval = QAFactEval(\n",
    "        cuda_device=0 if torch.cuda.is_available() else -1,\n",
    "        use_lerc_quip=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    def score_qafacteval(source, summary):\n",
    "        result = qafacteval.score_batch([source], [[summary]], return_qa_pairs=False)\n",
    "        return result[0]['qa_f1']\n",
    "    \n",
    "    metrics['QAFactEval'] = score_qafacteval\n",
    "    print(\"‚úì QAFactEval loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 SummaC-ZS\n",
    "if USE_SUMMAC:\n",
    "    print(\"Loading SummaC-ZS...\")\n",
    "    summac_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-large-mnli\")\n",
    "    summac_model = AutoModelForSequenceClassification.from_pretrained(\"FacebookAI/roberta-large-mnli\").to(device)\n",
    "    summac_model.eval()\n",
    "    \n",
    "    def score_summac(source, summary):\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sentences = sent_tokenize(summary)\n",
    "        if not sentences:\n",
    "            return 0.0\n",
    "        probs = []\n",
    "        for sent in sentences:\n",
    "            inputs = summac_tokenizer(source[:1024], sent, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = summac_model(**inputs)\n",
    "                p = torch.softmax(outputs.logits, dim=-1)\n",
    "                probs.append(p[0][2].item())  # Entailment\n",
    "        return float(np.mean(probs))\n",
    "    \n",
    "    metrics['SummaC'] = score_summac\n",
    "    print(\"‚úì SummaC loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 FactCC\n",
    "if USE_FACTCC:\n",
    "    print(\"Loading FactCC...\")\n",
    "    factcc_tokenizer = AutoTokenizer.from_pretrained(\"manueldeprada/FactCC\")\n",
    "    factcc_model = AutoModelForSequenceClassification.from_pretrained(\"manueldeprada/FactCC\").to(device)\n",
    "    factcc_model.eval()\n",
    "    \n",
    "    def score_factcc(source, summary):\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sentences = sent_tokenize(summary)\n",
    "        if not sentences:\n",
    "            return 0.0\n",
    "        probs = []\n",
    "        for sent in sentences:\n",
    "            inputs = factcc_tokenizer(source[:1024], sent, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = factcc_model(**inputs)\n",
    "                p = torch.softmax(outputs.logits, dim=-1)\n",
    "                probs.append(p[0][1].item())  # Consistent\n",
    "        return float(np.mean(probs))\n",
    "    \n",
    "    metrics['FactCC'] = score_factcc\n",
    "    print(\"‚úì FactCC loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 BARTScore\n",
    "if USE_BARTSCORE:\n",
    "    print(\"Loading BARTScore...\")\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "    bart_model.eval()\n",
    "    \n",
    "    def score_bartscore(source, summary):\n",
    "        source_ids = bart_tokenizer(source[:1024], return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
    "        with bart_tokenizer.as_target_tokenizer():\n",
    "            labels = bart_tokenizer(summary, return_tensors='pt', truncation=True, max_length=256).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = bart_model(input_ids=source_ids['input_ids'], attention_mask=source_ids['attention_mask'], labels=labels['input_ids'])\n",
    "            nll = outputs.loss.item()\n",
    "            score = -nll\n",
    "        return max(0, min(1, (score + 5) / 5))\n",
    "    \n",
    "    metrics['BARTScore'] = score_bartscore\n",
    "    print(\"‚úì BARTScore loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 BERTScore\n",
    "if USE_BERTSCORE:\n",
    "    print(\"Loading BERTScore...\")\n",
    "    from bert_score import score as bert_score_fn\n",
    "    \n",
    "    def score_bertscore(source, summary):\n",
    "        P, R, F1 = bert_score_fn([summary], [source[:2000]], model_type='microsoft/deberta-xlarge-mnli', device=device, verbose=False)\n",
    "        return F1.item()\n",
    "    \n",
    "    metrics['BERTScore'] = score_bertscore\n",
    "    print(\"‚úì BERTScore loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 UniEval-Fact\n",
    "if USE_UNIEVAL:\n",
    "    print(\"Loading UniEval-Fact...\")\n",
    "    unieval_tokenizer = AutoTokenizer.from_pretrained(\"MingZhong/unieval-fact\")\n",
    "    unieval_model = AutoModelForSeq2SeqLM.from_pretrained(\"MingZhong/unieval-fact\").to(device)\n",
    "    unieval_model.eval()\n",
    "    \n",
    "    def score_unieval(source, summary):\n",
    "        input_text = f\"question: Is this a factual summary? </s> document: {source[:500]} </s> summary: {summary}\"\n",
    "        inputs = unieval_tokenizer(input_text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = unieval_model.generate(inputs['input_ids'], max_length=10)\n",
    "            decoded = unieval_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"yes\" in decoded.lower():\n",
    "            return 1.0\n",
    "        elif \"no\" in decoded.lower():\n",
    "            return 0.0\n",
    "        return 0.5\n",
    "    \n",
    "    metrics['UniEval'] = score_unieval\n",
    "    print(\"‚úì UniEval loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 AlignScore\n",
    "if USE_ALIGNSCORE:\n",
    "    print(\"Loading AlignScore...\")\n",
    "    try:\n",
    "        align_tokenizer = AutoTokenizer.from_pretrained(\"yzha/AlignScore-base\")\n",
    "        align_model = AutoModelForSequenceClassification.from_pretrained(\"yzha/AlignScore-base\").to(device)\n",
    "        align_model.eval()\n",
    "        \n",
    "        def score_alignscore(source, summary):\n",
    "            inputs = align_tokenizer(source[:1024], summary, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = align_model(**inputs)\n",
    "                score = torch.sigmoid(outputs.logits).item()\n",
    "            return score\n",
    "        \n",
    "        metrics['AlignScore'] = score_alignscore\n",
    "        print(\"‚úì AlignScore loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† AlignScore not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 QAGS (using our implementation)\n",
    "if USE_QAGS:\n",
    "    print(\"Loading QAGS...\")\n",
    "    from transformers import pipeline\n",
    "    \n",
    "    qags_qg_tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "    qags_qg_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\").to(device)\n",
    "    qags_qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2', device=0 if torch.cuda.is_available() else -1)\n",
    "    \n",
    "    def score_qags(source, summary):\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sentences = sent_tokenize(summary)[:5]\n",
    "        questions = []\n",
    "        for sent in sentences:\n",
    "            inputs = qags_qg_tokenizer.encode(\"generate questions: \" + sent, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = qags_qg_model.generate(inputs, max_length=64, num_beams=4)\n",
    "                q = qags_qg_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                if q and \"?\" in q:\n",
    "                    questions.append(q)\n",
    "        if not questions:\n",
    "            return 0.0\n",
    "        matches = []\n",
    "        for q in questions:\n",
    "            try:\n",
    "                ans_src = qags_qa_pipeline(question=q, context=source[:2000], handle_impossible_answer=True)\n",
    "                ans_sum = qags_qa_pipeline(question=q, context=summary, handle_impossible_answer=True)\n",
    "                src_text = ans_src['answer'].lower().strip()\n",
    "                sum_text = ans_sum['answer'].lower().strip()\n",
    "                if not sum_text:\n",
    "                    continue\n",
    "                if src_text == sum_text or src_text in sum_text or sum_text in src_text:\n",
    "                    matches.append(1.0)\n",
    "                else:\n",
    "                    src_toks = set(src_text.split())\n",
    "                    sum_toks = set(sum_text.split())\n",
    "                    overlap = len(src_toks & sum_toks) / len(sum_toks) if sum_toks else 0\n",
    "                    matches.append(overlap)\n",
    "            except:\n",
    "                continue\n",
    "        return float(np.mean(matches)) if matches else 0.0\n",
    "    \n",
    "    metrics['QAGS'] = score_qags\n",
    "    print(\"‚úì QAGS loaded\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {len(metrics)} metrics: {list(metrics.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Summaries & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "from tqdm.notebook import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def generate_summary(model, tokenizer, text, model_type, max_length=256):\n",
    "    \"\"\"Generate summary from a model.\"\"\"\n",
    "    max_input = 1024 if 'pegasus' in model_type.lower() else 4096\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_input).to(device)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            min_length=50,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def evaluate_model(model_name, model, tokenizer, samples, metrics):\n",
    "    \"\"\"Evaluate a single model on all samples with all metrics.\"\"\"\n",
    "    results = {'summaries': [], 'rouge': [], 'metrics': {m: [] for m in metrics}}\n",
    "    \n",
    "    for sample in tqdm(samples, desc=f\"{model_name}\"):\n",
    "        source = sample['document']\n",
    "        reference = sample['summary']\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = generate_summary(model, tokenizer, source, model_name)\n",
    "        results['summaries'].append(summary)\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge_scores = rouge.score(reference, summary)\n",
    "        results['rouge'].append({\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        })\n",
    "        \n",
    "        # Factuality metrics\n",
    "        for name, metric_fn in metrics.items():\n",
    "            try:\n",
    "                score = metric_fn(source, summary)\n",
    "                results['metrics'][name].append(score)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† {model_name} {name} error: {e}\")\n",
    "                results['metrics'][name].append(0.0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PEGASUS\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATING PEGASUS-MULTI_NEWS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-multi_news\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-multi_news\").to(device)\n",
    "\n",
    "pegasus_results = evaluate_model(\"PEGASUS\", pegasus_model, pegasus_tokenizer, samples, metrics)\n",
    "\n",
    "# Free memory\n",
    "del pegasus_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PRIMERA\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATING PRIMERA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "primera_tokenizer = LEDTokenizer.from_pretrained(\"allenai/PRIMERA\")\n",
    "primera_model = LEDForConditionalGeneration.from_pretrained(\"allenai/PRIMERA\").to(device)\n",
    "\n",
    "primera_results = evaluate_model(\"PRIMERA\", primera_model, primera_tokenizer, samples, metrics)\n",
    "\n",
    "# Free memory\n",
    "del primera_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and save results\n",
    "all_results = {\n",
    "    'PEGASUS': pegasus_results,\n",
    "    'PRIMERA': primera_results\n",
    "}\n",
    "\n",
    "summary_table = {}\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    rouge_avg = {\n",
    "        'ROUGE-1': np.mean([r['rouge1'] for r in results['rouge']]),\n",
    "        'ROUGE-2': np.mean([r['rouge2'] for r in results['rouge']]),\n",
    "        'ROUGE-L': np.mean([r['rougeL'] for r in results['rouge']])\n",
    "    }\n",
    "    metric_avg = {name: np.mean(scores) for name, scores in results['metrics'].items()}\n",
    "    summary_table[model_name] = {**rouge_avg, **metric_avg}\n",
    "    \n",
    "    # Save detailed results\n",
    "    df = pd.DataFrame({\n",
    "        'generated': results['summaries'],\n",
    "        'rouge1': [r['rouge1'] for r in results['rouge']],\n",
    "        'rouge2': [r['rouge2'] for r in results['rouge']],\n",
    "        'rougeL': [r['rougeL'] for r in results['rouge']],\n",
    "        **{f'{m.lower()}_score': scores for m, scores in results['metrics'].items()}\n",
    "    })\n",
    "    df.to_csv(f\"{OUTPUT_DIR}/{model_name.lower()}_detailed_results.csv\", index=False)\n",
    "    print(f\"‚úì Saved {model_name} details\")\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/complete_comparison.json\", 'w') as f:\n",
    "    json.dump(summary_table, f, indent=2)\n",
    "print(f\"‚úì Saved comparison summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PUBLICATION-READY RESULTS TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Model':<12} {'R-1':>7} {'R-2':>7} {'R-L':>7}\", end=\"\")\n",
    "for m in metrics.keys():\n",
    "    print(f\" {m[:8]:>8}\", end=\"\")\n",
    "print()\n",
    "print(\"-\"*100)\n",
    "\n",
    "for model_name, scores in summary_table.items():\n",
    "    print(f\"{model_name:<12} {scores['ROUGE-1']:>7.4f} {scores['ROUGE-2']:>7.4f} {scores['ROUGE-L']:>7.4f}\", end=\"\")\n",
    "    for m in metrics.keys():\n",
    "        print(f\" {scores.get(m, 0):>8.4f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nüìÅ All results saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate LaTeX Table (For Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX table for paper\n",
    "latex = \"\"\"\\\\begin{table*}[t]\n",
    "\\\\centering\n",
    "\\\\caption{Comparison of summarization models on Multi-News test set (N=\"\"\" + str(NUM_SAMPLES) + \"\"\").}\n",
    "\\\\label{tab:results}\n",
    "\\\\begin{tabular}{l|ccc|\"\"\" + \"c\"*len(metrics) + \"\"\"}\n",
    "\\\\toprule\n",
    "Model & R-1 & R-2 & R-L\"\"\"\n",
    "\n",
    "for m in metrics.keys():\n",
    "    latex += f\" & {m}\"\n",
    "latex += \" \\\\\\\\ \\\\midrule\\n\"\n",
    "\n",
    "for model_name, scores in summary_table.items():\n",
    "    latex += f\"{model_name} & {scores['ROUGE-1']:.2f} & {scores['ROUGE-2']:.2f} & {scores['ROUGE-L']:.2f}\"\n",
    "    for m in metrics.keys():\n",
    "        latex += f\" & {scores.get(m, 0):.2f}\"\n",
    "    latex += \" \\\\\\\\\\n\"\n",
    "\n",
    "latex += \"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table*}\"\"\"\n",
    "\n",
    "print(latex)\n",
    "\n",
    "# Save LaTeX\n",
    "with open(f\"{OUTPUT_DIR}/results_table.tex\", 'w') as f:\n",
    "    f.write(latex)\n",
    "print(f\"\\n‚úì LaTeX table saved to {OUTPUT_DIR}/results_table.tex\")"
   ]
  }
 ]
}
