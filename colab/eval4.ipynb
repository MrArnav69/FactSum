{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Comprehensive Summarization Evaluation - Google Colab\n",
          "## PEGASUS vs PRIMERA on Multi-News Dataset\n",
          "### All Original Metrics for Top-Tier Publication\n",
          "\n",
          "**⚡ IMPORTANT: Set Runtime to GPU** (Runtime → Change runtime type → T4 GPU or better)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Check GPU\n",
          "!nvidia-smi"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Install core packages\n",
          "!pip install -q datasets transformers torch accelerate evaluate rouge-score bert-score pandas matplotlib seaborn scipy summac"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Clone metric repositories\n",
          "!rm -rf BARTScore UniEval qags QAFactEval AlignScore\n",
          "!git clone -q https://github.com/neulab/BARTScore.git\n",
          "!git clone -q https://github.com/maszhongming/UniEval.git\n",
          "!git clone -q https://github.com/W4ngatang/qags.git\n",
          "!git clone -q https://github.com/salesforce/QAFactEval.git\n",
          "!git clone -q https://github.com/yuh-zha/AlignScore.git\n",
          "\n",
          "# Install additional dependencies\n",
          "!pip install -q -r /content/UniEval/requirements.txt\n",
          "!pip install -q spacy nltk questeval\n",
          "!python -m spacy download en_core_web_sm\n",
          "\n",
          "# Download AlignScore checkpoint\n",
          "!wget -q https://huggingface.co/yzha/AlignScore/resolve/main/AlignScore-base.ckpt -O /content/AlignScore-base.ckpt\n",
          "\n",
          "# Add to path\n",
          "import sys\n",
          "sys.path.insert(0, '/content/BARTScore')\n",
          "sys.path.insert(0, '/content/UniEval')\n",
          "sys.path.insert(0, '/content/qags')\n",
          "sys.path.insert(0, '/content/QAFactEval')\n",
          "sys.path.insert(0, '/content/AlignScore')\n",
          "\n",
          "print(\"✓ All metrics installed\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Imports\n",
          "import os\n",
          "import torch\n",
          "import numpy as np\n",
          "import pandas as pd\n",
          "from tqdm.auto import tqdm\n",
          "from datasets import load_dataset\n",
          "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
          "import warnings\n",
          "warnings.filterwarnings('ignore')\n",
          "\n",
          "print(f\"PyTorch: {torch.__version__}\")\n",
          "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
          "if torch.cuda.is_available():\n",
          "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "class SummarizationEvaluator:\n",
          "    def __init__(self, num_samples=100):\n",
          "        self.num_samples = num_samples\n",
          "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
          "        print(f\"Device: {self.device}\")\n",
          "        \n",
          "        print(\"Loading Multi-News...\")\n",
          "        self.dataset = load_dataset(\"Awesome075/multi_news_parquet\", split='test')\n",
          "        self.test_data = self.dataset.select(range(min(num_samples, len(self.dataset))))\n",
          "        print(f\"Loaded {len(self.test_data)} samples\")\n",
          "        \n",
          "        self.models = {\n",
          "            'PEGASUS': 'google/pegasus-multi_news',\n",
          "            'PRIMERA': 'allenai/PRIMERA'\n",
          "        }\n",
          "        \n",
          "        self.setup_metrics()\n",
          "    \n",
          "    def setup_metrics(self):\n",
          "        from evaluate import load as eval_load\n",
          "        from bert_score import BERTScorer\n",
          "        \n",
          "        self.rouge = eval_load('rouge')\n",
          "        self.bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=self.device)\n",
          "        print(\"✓ ROUGE, BERTScore\")\n",
          "        \n",
          "        try:\n",
          "            from bart_score import BARTScorer\n",
          "            self.bart_scorer = BARTScorer(device=self.device, checkpoint='facebook/bart-large-cnn')\n",
          "            print(\"✓ BARTScore\")\n",
          "        except:\n",
          "            self.bart_scorer = None\n",
          "            print(\"✗ BARTScore failed\")\n",
          "        \n",
          "        try:\n",
          "            from summac.model_summac import SummaCZS\n",
          "            self.summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=self.device)\n",
          "            print(\"✓ SummaC\")\n",
          "        except:\n",
          "            self.summac_model = None\n",
          "            print(\"✗ SummaC failed\")\n",
          "        \n",
          "        try:\n",
          "            self.factcc_tokenizer = AutoTokenizer.from_pretrained(\"manueldeprada/FactCC\")\n",
          "            self.factcc_model = AutoModelForSequenceClassification.from_pretrained(\"manueldeprada/FactCC\")\n",
          "            self.factcc_model.to(self.device).eval()\n",
          "            print(\"✓ FactCC\")\n",
          "        except:\n",
          "            self.factcc_model = None\n",
          "            print(\"✗ FactCC failed\")\n",
          "        \n",
          "        try:\n",
          "            from metric.evaluator import get_evaluator\n",
          "            self.unieval = get_evaluator('summarization')\n",
          "            print(\"✓ UniEval\")\n",
          "        except:\n",
          "            self.unieval = None\n",
          "            print(\"✗ UniEval failed\")\n",
          "        \n",
          "        try:\n",
          "            from alignscore import AlignScore\n",
          "            self.alignscore = AlignScore(model='roberta-base', batch_size=32, device=self.device,\n",
          "                                        ckpt_path='/content/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
          "            print(\"✓ AlignScore\")\n",
          "        except:\n",
          "            self.alignscore = None\n",
          "            print(\"✗ AlignScore failed\")\n",
          "        \n",
          "        try:\n",
          "            from qags_model import QAGSModel\n",
          "            self.qags_model = QAGSModel()\n",
          "            print(\"✓ QAGS\")\n",
          "        except:\n",
          "            self.qags_model = None\n",
          "            print(\"✗ QAGS failed\")\n",
          "        \n",
          "        try:\n",
          "            from run_model import QAFactEval as QAFactEvalModel\n",
          "            self.qafacteval = QAFactEvalModel()\n",
          "            print(\"✓ QAFactEval\")\n",
          "        except:\n",
          "            self.qafacteval = None\n",
          "            print(\"✗ QAFactEval failed\")\n",
          "    \n",
          "    def generate_summaries(self, model_name, model_path):\n",
          "        print(f\"\\nGenerating {model_name} summaries...\")\n",
          "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
          "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device).eval()\n",
          "        \n",
          "        summaries = []\n",
          "        with torch.no_grad():\n",
          "            for item in tqdm(self.test_data):\n",
          "                inputs = tokenizer(item['document'], max_length=1024, truncation=True, return_tensors='pt').to(self.device)\n",
          "                summary_ids = model.generate(inputs['input_ids'])\n",
          "                summaries.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
          "        \n",
          "        del model\n",
          "        torch.cuda.empty_cache()\n",
          "        return summaries\n",
          "    \n",
          "    def evaluate_rouge(self, predictions, references):\n",
          "        r = self.rouge.compute(predictions=predictions, references=references)\n",
          "        return {'rouge1': r['rouge1'], 'rouge2': r['rouge2'], 'rougeL': r['rougeL']}\n",
          "    \n",
          "    def evaluate_bertscore(self, predictions, references):\n",
          "        P, R, F1 = self.bert_scorer.score(predictions, references)\n",
          "        return {'bertscore_p': P.mean().item(), 'bertscore_r': R.mean().item(), 'bertscore_f1': F1.mean().item()}\n",
          "    \n",
          "    def evaluate_bartscore(self, predictions, references, sources):\n",
          "        if not self.bart_scorer: return {'bartscore_ref': None, 'bartscore_src': None}\n",
          "        return {'bartscore_ref': np.mean(self.bart_scorer.score(references, predictions)),\n",
          "                'bartscore_src': np.mean(self.bart_scorer.score(sources, predictions))}\n",
          "    \n",
          "    def evaluate_summac(self, predictions, sources):\n",
          "        if not self.summac_model: return {'summac': None}\n",
          "        scores = [self.summac_model.score([src], [pred])['scores'][0] for pred, src in zip(predictions, sources)]\n",
          "        return {'summac': np.mean(scores)}\n",
          "    \n",
          "    def evaluate_factcc(self, predictions, sources):\n",
          "        if not self.factcc_model: return {'factcc': None}\n",
          "        scores = []\n",
          "        for pred, src in tqdm(zip(predictions, sources), desc=\"FactCC\", total=len(predictions)):\n",
          "            inputs = self.factcc_tokenizer(src, pred, max_length=512, truncation=True, return_tensors='pt').to(self.device)\n",
          "            with torch.no_grad():\n",
          "                logits = self.factcc_model(**inputs).logits\n",
          "                scores.append(torch.softmax(logits, dim=1)[0][1].item())\n",
          "        return {'factcc': np.mean(scores)}\n",
          "    \n",
          "    def evaluate_unieval(self, predictions, references, sources):\n",
          "        if not self.unieval: return {f'unieval_{k}': None for k in ['coherence', 'consistency', 'fluency', 'relevance', 'overall']}\n",
          "        data = [{'source': s, 'reference': r, 'system_output': p} for s, r, p in zip(sources, references, predictions)]\n",
          "        res = self.unieval.evaluate(data, dims=['coherence', 'consistency', 'fluency', 'relevance'], overall=True)\n",
          "        return {f'unieval_{k}': np.mean([r[k] for r in res]) for k in ['coherence', 'consistency', 'fluency', 'relevance', 'overall']}\n",
          "    \n",
          "    def evaluate_alignscore(self, predictions, sources):\n",
          "        if not self.alignscore: return {'alignscore': None}\n",
          "        return {'alignscore': np.mean(self.alignscore.score(contexts=sources, claims=predictions))}\n",
          "    \n",
          "    def evaluate_qags(self, predictions, sources):\n",
          "        if not self.qags_model: return {'qags': None}\n",
          "        scores = []\n",
          "        for pred, src in tqdm(zip(predictions, sources), desc=\"QAGS\", total=len(predictions)):\n",
          "            try: scores.append(self.qags_model.score(source=src, summary=pred))\n",
          "            except: pass\n",
          "        return {'qags': np.mean(scores) if scores else None}\n",
          "    \n",
          "    def evaluate_qafacteval(self, predictions, sources):\n",
          "        if not self.qafacteval: return {'qafacteval': None}\n",
          "        try: return {'qafacteval': np.mean(self.qafacteval.score_batch_qafacteval(sources=sources, summaries=predictions))}\n",
          "        except: return {'qafacteval': None}\n",
          "    \n",
          "    def run_evaluation(self):\n",
          "        all_results = {}\n",
          "        for model_name, model_path in self.models.items():\n",
          "            print(f\"\\n{'='*60}\\n{model_name}\\n{'='*60}\")\n",
          "            predictions = self.generate_summaries(model_name, model_path)\n",
          "            references = [item['summary'] for item in self.test_data]\n",
          "            sources = [item['document'] for item in self.test_data]\n",
          "            \n",
          "            results = {}\n",
          "            results.update(self.evaluate_rouge(predictions, references))\n",
          "            results.update(self.evaluate_bertscore(predictions, references))\n",
          "            results.update(self.evaluate_bartscore(predictions, references, sources))\n",
          "            results.update(self.evaluate_summac(predictions, sources))\n",
          "            results.update(self.evaluate_factcc(predictions, sources))\n",
          "            results.update(self.evaluate_unieval(predictions, references, sources))\n",
          "            results.update(self.evaluate_alignscore(predictions, sources))\n",
          "            results.update(self.evaluate_qags(predictions, sources))\n",
          "            results.update(self.evaluate_qafacteval(predictions, sources))\n",
          "            \n",
          "            all_results[model_name] = results\n",
          "            for k, v in results.items():\n",
          "                print(f\"{k:30s}: {v if v is None else f'{v:.4f}'}\")\n",
          "        return all_results\n",
          "    \n",
          "    def save_and_display(self, results):\n",
          "        import json\n",
          "        from google.colab import files\n",
          "        \n",
          "        # JSON\n",
          "        clean = {k: {m: (v if v is not None else \"N/A\") for m, v in r.items()} for k, r in results.items()}\n",
          "        with open('results.json', 'w') as f: json.dump(clean, f, indent=2)\n",
          "        \n",
          "        # DataFrame\n",
          "        df = pd.DataFrame(results).T.round(4)\n",
          "        df.to_csv('results.csv')\n",
          "        print(\"\\n\" + \"=\"*80 + \"\\nRESULTS\\n\" + \"=\"*80)\n",
          "        print(df.to_string())\n",
          "        \n",
          "        # LaTeX\n",
          "        with open('results.tex', 'w') as f: f.write(df.to_latex(float_format=\"%.4f\", na_rep=\"N/A\"))\n",
          "        \n",
          "        # Download\n",
          "        files.download('results.json')\n",
          "        files.download('results.csv')\n",
          "        files.download('results.tex')\n",
          "        \n",
          "        return df\n",
          "\n",
          "print(\"✓ Evaluator ready\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Quick Test (5 samples)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "test_eval = SummarizationEvaluator(num_samples=5)\n",
          "test_results = test_eval.run_evaluation()\n",
          "test_eval.save_and_display(test_results)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Full Evaluation (100 samples)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "evaluator = SummarizationEvaluator(num_samples=100)\n",
          "results = evaluator.run_evaluation()\n",
          "df = evaluator.save_and_display(results)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Visualization"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "import matplotlib.pyplot as plt\n",
          "import seaborn as sns\n",
          "from google.colab import files\n",
          "\n",
          "sns.set_style(\"whitegrid\")\n",
          "plot_df = df.dropna(axis=1)\n",
          "\n",
          "if len(plot_df.columns) > 0:\n",
          "    plt.figure(figsize=(16, 8))\n",
          "    plot_df.T.plot(kind='bar', width=0.75, edgecolor='black')\n",
          "    plt.title('PEGASUS vs PRIMERA', fontsize=16, fontweight='bold')\n",
          "    plt.xlabel('Metrics', fontsize=12)\n",
          "    plt.ylabel('Score', fontsize=12)\n",
          "    plt.xticks(rotation=45, ha='right')\n",
          "    plt.legend(title='Models')\n",
          "    plt.tight_layout()\n",
          "    plt.savefig('comparison.png', dpi=300)\n",
          "    plt.show()\n",
          "    files.download('comparison.png')"
        ]
      }
    ],
    "metadata": {
      "accelerator": "GPU",
      "colab": {
        "provenance": [],
        "gpuType": "T4"
      },
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "name": "python"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 0
  }