{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Summarization Evaluation\n",
    "## Evaluating PEGASUS and PRIMERA on Multi-News Dataset\n",
    "### Using All Original Metrics for Top-Tier Publication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Core Dependencies\n",
    "Start with the basic packages that install cleanly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch evaluate rouge-score bert-score pandas matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Advanced Metrics (Manual Installation)\n",
    "These require special handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SummaC (factual consistency)\n",
    "!pip install summac -q\n",
    "\n",
    "# Install BARTScore manually\n",
    "!git clone https://github.com/neulab/BARTScore.git\n",
    "import sys\n",
    "sys.path.append('./BARTScore')\n",
    "\n",
    "# Install UniEval manually\n",
    "!git clone https://github.com/maszhongming/UniEval.git\n",
    "sys.path.append('./UniEval')\n",
    "\n",
    "# Install additional requirements for UniEval\n",
    "!pip install -r ./UniEval/requirements.txt -q\n",
    "\n",
    "print(\"✓ Advanced metrics installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install FactCC, QAGS, QAFactEval, AlignScore\n",
    "These metrics need special installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FactCC - use transformers directly for the model\n",
    "print(\"FactCC will use transformers AutoModel...\")\n",
    "\n",
    "# QAGS - manual clone\n",
    "!git clone https://github.com/W4ngatang/qags.git\n",
    "sys.path.append('./qags')\n",
    "!pip install spacy nltk -q\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# QAFactEval - manual installation\n",
    "!git clone https://github.com/salesforce/QAFactEval.git\n",
    "sys.path.append('./QAFactEval')\n",
    "\n",
    "# AlignScore - manual installation\n",
    "!git clone https://github.com/yuh-zha/AlignScore.git\n",
    "sys.path.append('./AlignScore')\n",
    "!wget -q https://huggingface.co/yzha/AlignScore/resolve/main/AlignScore-base.ckpt\n",
    "\n",
    "print(\"✓ All metrics ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Class with Robust Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationEvaluator:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"Loading Multi-News dataset...\")\n",
    "        self.dataset = load_dataset(\"Awesome075/multi_news_parquet\", split='test')\n",
    "        self.test_data = self.dataset.select(range(min(num_samples, len(self.dataset))))\n",
    "        print(f\"Loaded {len(self.test_data)} samples\")\n",
    "        \n",
    "        # Models configuration\n",
    "        self.models = {\n",
    "            'PEGASUS': 'google/pegasus-multi_news',\n",
    "            'PRIMERA': 'allenai/PRIMERA'\n",
    "        }\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.setup_metrics()\n",
    "        \n",
    "    def setup_metrics(self):\n",
    "        \"\"\"Initialize all evaluation metrics\"\"\"\n",
    "        print(\"\\nSetting up evaluation metrics...\")\n",
    "        \n",
    "        # ROUGE\n",
    "        from evaluate import load as eval_load\n",
    "        self.rouge = eval_load('rouge')\n",
    "        print(\"✓ ROUGE loaded\")\n",
    "        \n",
    "        # BERTScore\n",
    "        from bert_score import BERTScorer\n",
    "        self.bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "        print(\"✓ BERTScore loaded\")\n",
    "        \n",
    "        # BARTScore\n",
    "        try:\n",
    "            sys.path.insert(0, './BARTScore')\n",
    "            from bart_score import BARTScorer\n",
    "            self.bart_scorer = BARTScorer(device=self.device, checkpoint='facebook/bart-large-cnn')\n",
    "            print(\"✓ BARTScore loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ BARTScore failed: {e}\")\n",
    "            self.bart_scorer = None\n",
    "        \n",
    "        # SummaC\n",
    "        try:\n",
    "            from summac.model_summac import SummaCZS\n",
    "            self.summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=self.device)\n",
    "            print(\"✓ SummaC loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ SummaC failed: {e}\")\n",
    "            self.summac_model = None\n",
    "        \n",
    "        # FactCC (using transformers directly)\n",
    "        try:\n",
    "            self.factcc_tokenizer = AutoTokenizer.from_pretrained(\"manueldeprada/FactCC\")\n",
    "            self.factcc_model = AutoModelForSequenceClassification.from_pretrained(\"manueldeprada/FactCC\")\n",
    "            self.factcc_model.to(self.device)\n",
    "            self.factcc_model.eval()\n",
    "            print(\"✓ FactCC loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ FactCC failed: {e}\")\n",
    "            self.factcc_model = None\n",
    "        \n",
    "        # UniEval\n",
    "        try:\n",
    "            sys.path.insert(0, './UniEval')\n",
    "            from metric.evaluator import get_evaluator\n",
    "            self.unieval = get_evaluator('summarization')\n",
    "            print(\"✓ UniEval loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ UniEval failed: {e}\")\n",
    "            self.unieval = None\n",
    "        \n",
    "        # AlignScore\n",
    "        try:\n",
    "            sys.path.insert(0, './AlignScore')\n",
    "            from alignscore import AlignScore\n",
    "            self.alignscore = AlignScore(model='roberta-base', batch_size=32, device=self.device,\n",
    "                                        ckpt_path='AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "            print(\"✓ AlignScore loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ AlignScore failed: {e}\")\n",
    "            self.alignscore = None\n",
    "        \n",
    "        # QAGS\n",
    "        try:\n",
    "            sys.path.insert(0, './qags')\n",
    "            from qags_model import QAGSModel\n",
    "            self.qags_model = QAGSModel()\n",
    "            print(\"✓ QAGS loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ QAGS failed: {e}\")\n",
    "            self.qags_model = None\n",
    "        \n",
    "        # QAFactEval\n",
    "        try:\n",
    "            sys.path.insert(0, './QAFactEval')\n",
    "            from run_model import QAFactEval as QAFactEvalModel\n",
    "            self.qafacteval = QAFactEvalModel()\n",
    "            print(\"✓ QAFactEval loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ QAFactEval failed: {e}\")\n",
    "            self.qafacteval = None\n",
    "        \n",
    "        print(\"\\n✓ Metric initialization complete!\")\n",
    "        \n",
    "    def generate_summaries(self, model_name, model_path):\n",
    "        \"\"\"Generate summaries using the specified model\"\"\"\n",
    "        print(f\"\\nGenerating summaries with {model_name}...\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        summaries = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(self.test_data, desc=f\"Generating {model_name}\"):\n",
    "                source = item['document']\n",
    "                inputs = tokenizer(source, max_length=1024, truncation=True, \n",
    "                                 return_tensors='pt').to(self.device)\n",
    "                summary_ids = model.generate(inputs['input_ids'])\n",
    "                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                summaries.append(summary)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        return summaries\n",
    "    \n",
    "    def evaluate_rouge(self, predictions, references):\n",
    "        results = self.rouge.compute(predictions=predictions, references=references)\n",
    "        return {\n",
    "            'rouge1': results['rouge1'],\n",
    "            'rouge2': results['rouge2'],\n",
    "            'rougeL': results['rougeL']\n",
    "        }\n",
    "    \n",
    "    def evaluate_bertscore(self, predictions, references):\n",
    "        P, R, F1 = self.bert_scorer.score(predictions, references)\n",
    "        return {\n",
    "            'bertscore_precision': P.mean().item(),\n",
    "            'bertscore_recall': R.mean().item(),\n",
    "            'bertscore_f1': F1.mean().item()\n",
    "        }\n",
    "    \n",
    "    def evaluate_bartscore(self, predictions, references, sources):\n",
    "        if self.bart_scorer is None:\n",
    "            return {'bartscore_ref': None, 'bartscore_src': None}\n",
    "        ref_scores = self.bart_scorer.score(references, predictions)\n",
    "        src_scores = self.bart_scorer.score(sources, predictions)\n",
    "        return {\n",
    "            'bartscore_ref': np.mean(ref_scores),\n",
    "            'bartscore_src': np.mean(src_scores)\n",
    "        }\n",
    "    \n",
    "    def evaluate_summac(self, predictions, sources):\n",
    "        if self.summac_model is None:\n",
    "            return {'summac': None}\n",
    "        scores = []\n",
    "        for pred, src in tqdm(zip(predictions, sources), total=len(predictions), desc=\"SummaC\"):\n",
    "            score = self.summac_model.score([src], [pred])\n",
    "            scores.append(score['scores'][0])\n",
    "        return {'summac': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_factcc(self, predictions, sources):\n",
    "        if self.factcc_model is None:\n",
    "            return {'factcc': None}\n",
    "        scores = []\n",
    "        for pred, src in tqdm(zip(predictions, sources), total=len(predictions), desc=\"FactCC\"):\n",
    "            inputs = self.factcc_tokenizer(src, pred, max_length=512, truncation=True,\n",
    "                                          return_tensors='pt').to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.factcc_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                prob = torch.softmax(logits, dim=1)[0][1].item()\n",
    "                scores.append(prob)\n",
    "        return {'factcc': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_unieval(self, predictions, references, sources):\n",
    "        if self.unieval is None:\n",
    "            return {'unieval_coherence': None, 'unieval_consistency': None,\n",
    "                   'unieval_fluency': None, 'unieval_relevance': None, 'unieval_overall': None}\n",
    "        data = [{'source': src, 'reference': ref, 'system_output': pred}\n",
    "                for src, ref, pred in zip(sources, references, predictions)]\n",
    "        eval_results = self.unieval.evaluate(data, dims=['coherence', 'consistency', 'fluency', 'relevance'],\n",
    "                                            overall=True)\n",
    "        return {\n",
    "            'unieval_coherence': np.mean([r['coherence'] for r in eval_results]),\n",
    "            'unieval_consistency': np.mean([r['consistency'] for r in eval_results]),\n",
    "            'unieval_fluency': np.mean([r['fluency'] for r in eval_results]),\n",
    "            'unieval_relevance': np.mean([r['relevance'] for r in eval_results]),\n",
    "            'unieval_overall': np.mean([r['overall'] for r in eval_results])\n",
    "        }\n",
    "    \n",
    "    def evaluate_alignscore(self, predictions, sources):\n",
    "        if self.alignscore is None:\n",
    "            return {'alignscore': None}\n",
    "        scores = self.alignscore.score(contexts=sources, claims=predictions)\n",
    "        return {'alignscore': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_qags(self, predictions, sources):\n",
    "        if self.qags_model is None:\n",
    "            return {'qags': None}\n",
    "        scores = []\n",
    "        for pred, src in tqdm(zip(predictions, sources), total=len(predictions), desc=\"QAGS\"):\n",
    "            try:\n",
    "                score = self.qags_model.score(source=src, summary=pred)\n",
    "                scores.append(score)\n",
    "            except:\n",
    "                continue\n",
    "        return {'qags': np.mean(scores) if scores else None}\n",
    "    \n",
    "    def evaluate_qafacteval(self, predictions, sources):\n",
    "        if self.qafacteval is None:\n",
    "            return {'qafacteval': None}\n",
    "        try:\n",
    "            results = self.qafacteval.score_batch_qafacteval(sources=sources, summaries=predictions)\n",
    "            return {'qafacteval': np.mean(results)}\n",
    "        except:\n",
    "            return {'qafacteval': None}\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model_path in self.models.items():\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Evaluating {model_name}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            predictions = self.generate_summaries(model_name, model_path)\n",
    "            references = [item['summary'] for item in self.test_data]\n",
    "            sources = [item['document'] for item in self.test_data]\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            print(\"\\n[1/9] ROUGE...\")\n",
    "            results.update(self.evaluate_rouge(predictions, references))\n",
    "            \n",
    "            print(\"[2/9] BERTScore...\")\n",
    "            results.update(self.evaluate_bertscore(predictions, references))\n",
    "            \n",
    "            print(\"[3/9] BARTScore...\")\n",
    "            results.update(self.evaluate_bartscore(predictions, references, sources))\n",
    "            \n",
    "            print(\"[4/9] SummaC...\")\n",
    "            results.update(self.evaluate_summac(predictions, sources))\n",
    "            \n",
    "            print(\"[5/9] FactCC...\")\n",
    "            results.update(self.evaluate_factcc(predictions, sources))\n",
    "            \n",
    "            print(\"[6/9] UniEval...\")\n",
    "            results.update(self.evaluate_unieval(predictions, references, sources))\n",
    "            \n",
    "            print(\"[7/9] AlignScore...\")\n",
    "            results.update(self.evaluate_alignscore(predictions, sources))\n",
    "            \n",
    "            print(\"[8/9] QAGS...\")\n",
    "            results.update(self.evaluate_qags(predictions, sources))\n",
    "            \n",
    "            print(\"[9/9] QAFactEval...\")\n",
    "            results.update(self.evaluate_qafacteval(predictions, sources))\n",
    "            \n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            print(f\"\\n{model_name} Results:\")\n",
    "            for metric, value in results.items():\n",
    "                if value is not None:\n",
    "                    print(f\"  {metric:30s}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric:30s}: N/A (metric unavailable)\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def save_results(self, results, filename):\n",
    "        import json\n",
    "        # Convert None to string for JSON serialization\n",
    "        results_clean = {k: {mk: (mv if mv is not None else \"N/A\") \n",
    "                            for mk, mv in v.items()} for k, v in results.items()}\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_clean, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def create_results_table(self, results):\n",
    "        df = pd.DataFrame(results).T\n",
    "        metric_order = [\n",
    "            'rouge1', 'rouge2', 'rougeL',\n",
    "            'bertscore_precision', 'bertscore_recall', 'bertscore_f1',\n",
    "            'bartscore_ref', 'bartscore_src',\n",
    "            'summac', 'factcc', 'alignscore',\n",
    "            'unieval_coherence', 'unieval_consistency', 'unieval_fluency',\n",
    "            'unieval_relevance', 'unieval_overall',\n",
    "            'qags', 'qafacteval'\n",
    "        ]\n",
    "        df = df[[col for col in metric_order if col in df.columns]]\n",
    "        df = df.round(4)\n",
    "        df.to_csv('results_table.csv')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"FINAL RESULTS TABLE\")\n",
    "        print(\"=\"*90)\n",
    "        print(df.to_string())\n",
    "        print(\"\\nTable saved to results_table.csv\")\n",
    "        \n",
    "        latex_table = df.to_latex(float_format=\"%.4f\", na_rep=\"N/A\")\n",
    "        with open('results_table.tex', 'w') as f:\n",
    "            f.write(latex_table)\n",
    "        print(\"LaTeX table saved to results_table.tex\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = SummarizationEvaluator(num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Complete Evaluation\n",
    "⚠️ This will take several hours. Consider starting with fewer samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluator.run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_results(results, 'final_results.json')\n",
    "results_df = evaluator.create_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "# Remove None values for plotting\n",
    "plot_df = results_df.dropna(axis=1)\n",
    "\n",
    "plot_df.T.plot(kind='bar', width=0.75, edgecolor='black', linewidth=0.5)\n",
    "plt.title('PEGASUS vs PRIMERA: Comprehensive Evaluation', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Evaluation Metrics', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Score', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Models', fontsize=11, title_fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved to model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model in results_df.index:\n",
    "    print(f\"\\n{model}:\")\n",
    "    model_results = results_df.loc[model].dropna()\n",
    "    print(f\"  Available metrics: {len(model_results)}\")\n",
    "    print(f\"  Mean score: {model_results.mean():.4f}\")\n",
    "    print(f\"  Std dev: {model_results.std():.4f}\")\n",
    "    print(f\"  Min score: {model_results.min():.4f} ({model_results.idxmin()})\")\n",
    "    print(f\"  Max score: {model_results.max():.4f} ({model_results.idxmax()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for Publication\n",
    "\n",
    "This notebook implements comprehensive evaluation using:\n",
    "\n",
    "**Standard Reference-Based Metrics:**\n",
    "- ROUGE-1, ROUGE-2, ROUGE-L (Lin, 2004)\n",
    "- BERTScore (Zhang et al., 2020)\n",
    "\n",
    "**Factual Consistency Metrics:**\n",
    "- BARTScore (Yuan et al., 2021)\n",
    "- SummaC (Laban et al., 2022)\n",
    "- FactCC (Kryscinski et al., 2020)\n",
    "- AlignScore (Zha et al., 2023)\n",
    "- QAGS (Wang et al., 2020)\n",
    "- QAFactEval (Fabbri et al., 2022)\n",
    "\n",
    "**Multi-Dimensional Quality Metrics:**\n",
    "- UniEval (Zhong et al., 2022) - Coherence, Consistency, Fluency, Relevance\n",
    "\n",
    "All metrics are computed using their original implementations without modifications.\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If certain metrics fail to load, the evaluation will continue with available metrics\n",
    "- Results marked as \"N/A\" indicate the metric was unavailable\n",
    "- For publication, report only successfully computed metrics with proper citations\n",
    "\n",
    "**Citation Format for Paper:**\n",
    "```\n",
    "We evaluate model performance using multiple automatic metrics:\n",
    "- Reference-based: ROUGE (Lin, 2004), BERTScore (Zhang et al., 2020)\n",
    "- Factual consistency: BARTScore (Yuan et al., 2021), SummaC (Laban et al., 2022), \n",
    "  FactCC (Kryscinski et al., 2020), AlignScore (Zha et al., 2023)\n",
    "- QA-based: QAGS (Wang et al., 2020), QAFactEval (Fabbri et al., 2022)\n",
    "- Multi-dimensional: UniEval (Zhong et al., 2022)\n",
    "```"