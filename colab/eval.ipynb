{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Summarization Evaluation\n",
    "## Evaluating PEGASUS and PRIMERA on Multi-News Dataset\n",
    "### For Publication in Top-Tier Journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "Run this cell first to install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch evaluate rouge-score bert-score -q\n",
    "!pip install summac -q\n",
    "!pip install git+https://github.com/neulab/BARTScore.git -q\n",
    "!pip install git+https://github.com/maszhongming/UniEval.git -q\n",
    "!pip install git+https://github.com/W4ngatang/qags.git -q\n",
    "!pip install factcc -q\n",
    "!pip install qafacteval -q\n",
    "!pip install alignscore -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download AlignScore Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://huggingface.co/yzha/AlignScore/resolve/main/AlignScore-base.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationEvaluator:\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load dataset\n",
    "        print(\"Loading Multi-News dataset...\")\n",
    "        self.dataset = load_dataset(\"Awesome075/multi_news_parquet\", split='test')\n",
    "        self.test_data = self.dataset.select(range(min(num_samples, len(self.dataset))))\n",
    "        print(f\"Loaded {len(self.test_data)} samples\")\n",
    "        \n",
    "        # Models configuration\n",
    "        self.models = {\n",
    "            'PEGASUS': 'google/pegasus-multi_news',\n",
    "            'PRIMERA': 'allenai/PRIMERA'\n",
    "        }\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.setup_metrics()\n",
    "        \n",
    "    def setup_metrics(self):\n",
    "        \"\"\"Initialize all evaluation metrics\"\"\"\n",
    "        print(\"Setting up evaluation metrics...\")\n",
    "        \n",
    "        # ROUGE (Rouge-1, Rouge-2, Rouge-L)\n",
    "        from evaluate import load\n",
    "        self.rouge = load('rouge')\n",
    "        print(\"✓ ROUGE loaded\")\n",
    "        \n",
    "        # BERTScore\n",
    "        from bert_score import BERTScorer\n",
    "        self.bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "        print(\"✓ BERTScore loaded\")\n",
    "        \n",
    "        # BARTScore\n",
    "        from bart_score import BARTScorer\n",
    "        self.bart_scorer = BARTScorer(device=self.device, checkpoint='facebook/bart-large-cnn')\n",
    "        print(\"✓ BARTScore loaded\")\n",
    "        \n",
    "        # SummaC\n",
    "        from summac.model_summac import SummaCZS\n",
    "        self.summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=self.device)\n",
    "        print(\"✓ SummaC loaded\")\n",
    "        \n",
    "        # FactCC\n",
    "        from factcc.modeling import FactCCModel\n",
    "        self.factcc_model = FactCCModel.from_pretrained(\"manueldeprada/FactCC\")\n",
    "        self.factcc_model.to(self.device)\n",
    "        self.factcc_model.eval()\n",
    "        print(\"✓ FactCC loaded\")\n",
    "        \n",
    "        # UniEval\n",
    "        from utils import convert_to_json\n",
    "        from metric.evaluator import get_evaluator\n",
    "        self.unieval = get_evaluator('summarization')\n",
    "        print(\"✓ UniEval loaded\")\n",
    "        \n",
    "        # AlignScore\n",
    "        from alignscore import AlignScore\n",
    "        self.alignscore = AlignScore(model='roberta-base', batch_size=32, device=self.device, \n",
    "                                     ckpt_path='AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "        print(\"✓ AlignScore loaded\")\n",
    "        \n",
    "        # QAGS\n",
    "        from qags.qags_model import QAGSModel\n",
    "        self.qags_model = QAGSModel()\n",
    "        print(\"✓ QAGS loaded\")\n",
    "        \n",
    "        # QAFactEval\n",
    "        from QAFactEval.run_model import QAFactEval as QAFactEvalModel\n",
    "        self.qafacteval = QAFactEvalModel()\n",
    "        print(\"✓ QAFactEval loaded\")\n",
    "        \n",
    "        print(\"\\nAll metrics initialized successfully!\")\n",
    "        \n",
    "    def generate_summaries(self, model_name, model_path):\n",
    "        \"\"\"Generate summaries using the specified model\"\"\"\n",
    "        print(f\"\\nGenerating summaries with {model_name}...\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        summaries = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(self.test_data, desc=f\"Generating {model_name}\"):\n",
    "                source = item['document']\n",
    "                \n",
    "                # Tokenize input\n",
    "                inputs = tokenizer(source, max_length=1024, truncation=True, \n",
    "                                 return_tensors='pt').to(self.device)\n",
    "                \n",
    "                # Generate summary (vanilla generation - no special parameters)\n",
    "                summary_ids = model.generate(inputs['input_ids'])\n",
    "                \n",
    "                # Decode summary\n",
    "                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                summaries.append(summary)\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return summaries\n",
    "    \n",
    "    def evaluate_rouge(self, predictions, references):\n",
    "        \"\"\"Evaluate ROUGE-1, ROUGE-2, ROUGE-L\"\"\"\n",
    "        results = self.rouge.compute(predictions=predictions, references=references)\n",
    "        return {\n",
    "            'rouge1': results['rouge1'],\n",
    "            'rouge2': results['rouge2'],\n",
    "            'rougeL': results['rougeL']\n",
    "        }\n",
    "    \n",
    "    def evaluate_bertscore(self, predictions, references):\n",
    "        \"\"\"Evaluate BERTScore\"\"\"\n",
    "        P, R, F1 = self.bert_scorer.score(predictions, references)\n",
    "        return {\n",
    "            'bertscore_precision': P.mean().item(),\n",
    "            'bertscore_recall': R.mean().item(),\n",
    "            'bertscore_f1': F1.mean().item()\n",
    "        }\n",
    "    \n",
    "    def evaluate_bartscore(self, predictions, references, sources):\n",
    "        \"\"\"Evaluate BARTScore\"\"\"\n",
    "        # BARTScore ref-hyp (reference-based)\n",
    "        ref_scores = self.bart_scorer.score(references, predictions)\n",
    "        \n",
    "        # BARTScore src-hyp (source-based faithfulness)\n",
    "        src_scores = self.bart_scorer.score(sources, predictions)\n",
    "        \n",
    "        return {\n",
    "            'bartscore_ref': np.mean(ref_scores),\n",
    "            'bartscore_src': np.mean(src_scores)\n",
    "        }\n",
    "    \n",
    "    def evaluate_summac(self, predictions, sources):\n",
    "        \"\"\"Evaluate SummaC (factual consistency)\"\"\"\n",
    "        scores = []\n",
    "        for pred, src in zip(predictions, sources):\n",
    "            score = self.summac_model.score([src], [pred])\n",
    "            scores.append(score['scores'][0])\n",
    "        \n",
    "        return {'summac': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_factcc(self, predictions, sources):\n",
    "        \"\"\"Evaluate FactCC\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for pred, src in tqdm(zip(predictions, sources), total=len(predictions), desc=\"FactCC\"):\n",
    "            # Prepare input\n",
    "            inputs = self.factcc_model.tokenizer(\n",
    "                src, pred, max_length=512, truncation=True, \n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.factcc_model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                prob = torch.softmax(logits, dim=1)[0][1].item()  # Probability of being factual\n",
    "                scores.append(prob)\n",
    "        \n",
    "        return {'factcc': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_unieval(self, predictions, references, sources):\n",
    "        \"\"\"Evaluate UniEval (coherence, consistency, fluency, relevance)\"\"\"\n",
    "        # Prepare data in UniEval format\n",
    "        data = []\n",
    "        for src, ref, pred in zip(sources, references, predictions):\n",
    "            data.append({\n",
    "                'source': src,\n",
    "                'reference': ref,\n",
    "                'system_output': pred\n",
    "            })\n",
    "        \n",
    "        # Evaluate all dimensions\n",
    "        eval_results = self.unieval.evaluate(data, dims=['coherence', 'consistency', \n",
    "                                                         'fluency', 'relevance'], \n",
    "                                            overall=True)\n",
    "        \n",
    "        return {\n",
    "            'unieval_coherence': np.mean([r['coherence'] for r in eval_results]),\n",
    "            'unieval_consistency': np.mean([r['consistency'] for r in eval_results]),\n",
    "            'unieval_fluency': np.mean([r['fluency'] for r in eval_results]),\n",
    "            'unieval_relevance': np.mean([r['relevance'] for r in eval_results]),\n",
    "            'unieval_overall': np.mean([r['overall'] for r in eval_results])\n",
    "        }\n",
    "    \n",
    "    def evaluate_alignscore(self, predictions, sources):\n",
    "        \"\"\"Evaluate AlignScore (factual consistency)\"\"\"\n",
    "        scores = self.alignscore.score(contexts=sources, claims=predictions)\n",
    "        return {'alignscore': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_qags(self, predictions, sources):\n",
    "        \"\"\"Evaluate QAGS\"\"\"\n",
    "        scores = []\n",
    "        for pred, src in tqdm(zip(predictions, sources), total=len(predictions), desc=\"QAGS\"):\n",
    "            score = self.qags_model.score(source=src, summary=pred)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return {'qags': np.mean(scores)}\n",
    "    \n",
    "    def evaluate_qafacteval(self, predictions, sources):\n",
    "        \"\"\"Evaluate QAFactEval\"\"\"\n",
    "        results = self.qafacteval.score_batch_qafacteval(\n",
    "            sources=sources,\n",
    "            summaries=predictions\n",
    "        )\n",
    "        \n",
    "        return {'qafacteval': np.mean(results)}\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model_path in self.models.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Evaluating {model_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Generate summaries\n",
    "            predictions = self.generate_summaries(model_name, model_path)\n",
    "            \n",
    "            # Extract references and sources\n",
    "            references = [item['summary'] for item in self.test_data]\n",
    "            sources = [item['document'] for item in self.test_data]\n",
    "            \n",
    "            # Run all metrics\n",
    "            results = {}\n",
    "            \n",
    "            print(\"\\nEvaluating ROUGE...\")\n",
    "            results.update(self.evaluate_rouge(predictions, references))\n",
    "            \n",
    "            print(\"Evaluating BERTScore...\")\n",
    "            results.update(self.evaluate_bertscore(predictions, references))\n",
    "            \n",
    "            print(\"Evaluating BARTScore...\")\n",
    "            results.update(self.evaluate_bartscore(predictions, references, sources))\n",
    "            \n",
    "            print(\"Evaluating SummaC...\")\n",
    "            results.update(self.evaluate_summac(predictions, sources))\n",
    "            \n",
    "            print(\"Evaluating FactCC...\")\n",
    "            results.update(self.evaluate_factcc(predictions, sources))\n",
    "            \n",
    "            print(\"Evaluating UniEval...\")\n",
    "            results.update(self.evaluate_unieval(predictions, references, sources))\n",
    "            \n",
    "            print(\"Evaluating AlignScore...\")\n",
    "            results.update(self.evaluate_alignscore(predictions, sources))\n",
    "            \n",
    "            print(\"Evaluating QAGS...\")\n",
    "            results.update(self.evaluate_qags(predictions, sources))\n",
    "            \n",
    "            print(\"Evaluating QAFactEval...\")\n",
    "            results.update(self.evaluate_qafacteval(predictions, sources))\n",
    "            \n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Display results for this model\n",
    "            print(f\"\\n{model_name} Results:\")\n",
    "            for metric, value in results.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def save_results(self, results, filename):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        import json\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\nResults saved to {filename}\")\n",
    "    \n",
    "    def create_results_table(self, results):\n",
    "        \"\"\"Create formatted results table for publication\"\"\"\n",
    "        df = pd.DataFrame(results).T\n",
    "        \n",
    "        # Reorder columns for better presentation\n",
    "        metric_order = [\n",
    "            'rouge1', 'rouge2', 'rougeL',\n",
    "            'bertscore_precision', 'bertscore_recall', 'bertscore_f1',\n",
    "            'bartscore_ref', 'bartscore_src',\n",
    "            'summac', 'factcc', 'alignscore',\n",
    "            'unieval_coherence', 'unieval_consistency', 'unieval_fluency', \n",
    "            'unieval_relevance', 'unieval_overall',\n",
    "            'qags', 'qafacteval'\n",
    "        ]\n",
    "        \n",
    "        df = df[[col for col in metric_order if col in df.columns]]\n",
    "        \n",
    "        # Round values for publication\n",
    "        df = df.round(4)\n",
    "        \n",
    "        # Save as CSV\n",
    "        df.to_csv('results_table.csv')\n",
    "        \n",
    "        # Print formatted table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FINAL RESULTS TABLE\")\n",
    "        print(\"=\"*80)\n",
    "        print(df.to_string())\n",
    "        print(\"\\nTable saved to results_table.csv\")\n",
    "        \n",
    "        # Create LaTeX table for publication\n",
    "        latex_table = df.to_latex(float_format=\"%.4f\")\n",
    "        with open('results_table.tex', 'w') as f:\n",
    "            f.write(latex_table)\n",
    "        print(\"LaTeX table saved to results_table.tex\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Evaluator\n",
    "This will load the dataset and initialize all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator with 100 test samples\n",
    "evaluator = SummarizationEvaluator(num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Complete Evaluation\n",
    "This will evaluate both PEGASUS and PRIMERA on all metrics. This may take several hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = evaluator.run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "evaluator.save_results(results, 'final_results.json')\n",
    "\n",
    "# Create and display results table\n",
    "results_df = evaluator.create_results_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display Individual Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PEGASUS results\n",
    "print(\"PEGASUS Results:\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in results['PEGASUS'].items():\n",
    "    print(f\"{metric:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PRIMERA results\n",
    "print(\"PRIMERA Results:\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in results['PRIMERA'].items():\n",
    "    print(f\"{metric:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Create comparison plot\n",
    "results_df_T = results_df.T\n",
    "results_df_T.plot(kind='bar', width=0.8)\n",
    "plt.title('Model Comparison Across All Metrics', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Models', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved to model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Statistical Significance Testing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have per-sample scores, you can perform statistical tests\n",
    "# This is a placeholder for paired t-test or Wilcoxon signed-rank test\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Example: Compare ROUGE-1 scores (you would need per-sample scores)\n",
    "# pegasus_rouge1_scores = [...]\n",
    "# primera_rouge1_scores = [...]\n",
    "# t_stat, p_value = stats.ttest_rel(pegasus_rouge1_scores, primera_rouge1_scores)\n",
    "# print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "print(\"Statistical testing can be added here with per-sample scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluates PEGASUS and PRIMERA models on the Multi-News dataset using:\n",
    "\n",
    "**Reference-based Metrics:**\n",
    "- ROUGE-1, ROUGE-2, ROUGE-L\n",
    "- BERTScore (Precision, Recall, F1)\n",
    "- BARTScore (Reference-based)\n",
    "\n",
    "**Factual Consistency Metrics:**\n",
    "- BARTScore (Source-based)\n",
    "- SummaC\n",
    "- FactCC\n",
    "- AlignScore\n",
    "- QAGS\n",
    "- QAFactEval\n",
    "\n",
    "**Multi-dimensional Metrics:**\n",
    "- UniEval (Coherence, Consistency, Fluency, Relevance, Overall)\n",
    "\n",
    "All results are saved in publication-ready formats (CSV and LaTeX)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}