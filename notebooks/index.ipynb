{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066d745a",
   "metadata": {},
   "source": [
    "# FactSum: Fact-Verification-Guided Abstractive Summarizer for News Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48ba9f",
   "metadata": {},
   "source": [
    "## Loading Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a194539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/factsum/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset #For the Multi-News dataset\n",
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaf04e",
   "metadata": {},
   "source": [
    "# Loading the main model:\n",
    "- **pegasus-multi_news**: A pre-trained model fine-tuned on the Multi-News dataset for abstractive summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27ac505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-multi_news and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model Identifier\n",
    "MODEL_NAME = \"google/pegasus-multi_news\"\n",
    "\n",
    "# Loading the tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370ac81",
   "metadata": {},
   "source": [
    "# Loading the Dataset(s):\n",
    "- **Multi-News**: A dataset containing news articles and their corresponding human-written summaries, used for training and evaluating summarization models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61011fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_news = load_dataset(\"Awesome075/multi_news_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aac6c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 44972\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 5622\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 5622\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(multi_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a05257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      " GOP Eyes Gains As Voters In 11 States Pick Governors \n",
      " \n",
      " Enlarge this image toggle caption Jim Cole/AP Jim Cole/AP \n",
      " \n",
      " Voters in 11 states will pick their governors tonight, and Republicans appear on track to increase their numbers by at least one, with the potential to extend their hold to more than two-thirds of the nation's top state offices. \n",
      " \n",
      " Eight of the gubernatorial seats up for grabs are now held by Democrats; three are in Republican hands. Republicans currently hold 29 governorships, Democrats have 20, and Rhode Island's Gov. Lincoln Chafee is an Independent. \n",
      " \n",
      " Polls and race analysts suggest that only three of tonight's contests are considered competitive, all in states where incumbent Democratic governors aren't running again: Montana, New Hampshire and Washington. \n",
      " \n",
      " While those state races remain too close to call, Republicans are expected to wrest the North Carolina governorship from Democratic control, and to easily win GOP-held seats in Utah, North Dakota and Indi ...\n",
      "\n",
      "SUMMARY:\n",
      " – It's a race for the governor's mansion in 11 states today, and the GOP could end the night at the helm of more than two-thirds of the 50 states. The GOP currently controls 29 of the country's top state offices; it's expected to keep the three Republican ones that are up for grabs (Utah, North Dakota, and Indiana), and wrest North Carolina from the Dems. That brings its toll to 30, with the potential to take three more, reports NPR. Races in Montana, New Hampshire, and Washington are still too close to call, and in all three, Democrat incumbents aren't seeking reelection. The results could have a big impact on health care, since a Supreme Court ruling grants states the ability to opt out of ObamaCare's Medicaid expansion. \"A Romney victory would dramatically empower Republican governors,\" said one analyst. Click for NPR's state-by-state breakdown of what could happen.\n"
     ]
    }
   ],
   "source": [
    "example = multi_news[\"test\"][0]\n",
    "\n",
    "print(\"DOCUMENT:\\n\", example[\"document\"][:1000], \"...\")  # first 1000 chars\n",
    "print(\"\\nSUMMARY:\\n\", example[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f0686",
   "metadata": {},
   "source": [
    "# Implementing the Chunking (1024 tokens) with 128 token overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e432988f",
   "metadata": {},
   "source": [
    "## Semantic Document Chunker Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f69bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from transformers import PegasusTokenizer\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class SemanticDocumentChunker:\n",
    "    \"\"\"\n",
    "    State-of-the-art semantic document chunker for PEGASUS-based multi-document\n",
    "    summarization with intelligent boundary detection and adaptive overlap.\n",
    "    \n",
    "    Key innovations:\n",
    "    1. Sentence-boundary preservation (never split mid-sentence)\n",
    "    2. Semantic coherence via sentence embeddings (optional)\n",
    "    3. Adaptive overlap based on content similarity\n",
    "    4. Paragraph and discourse-aware chunking\n",
    "    5. Comprehensive metadata tracking for reproducibility\n",
    "    \n",
    "    Designed for the FactSum framework (Gupta, 2025).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"google/pegasus-multi_news\", \n",
    "                 max_tokens: int = 1024, \n",
    "                 overlap_tokens: int = 128,\n",
    "                 use_sentence_boundaries: bool = True,\n",
    "                 min_chunk_tokens: int = 256,\n",
    "                 preserve_paragraphs: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the semantic chunker.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            max_tokens: Maximum tokens per chunk (soft limit with sentence boundaries)\n",
    "            overlap_tokens: Target overlap tokens (adaptive)\n",
    "            use_sentence_boundaries: Never split sentences across chunks\n",
    "            min_chunk_tokens: Minimum tokens per chunk (avoid tiny chunks)\n",
    "            preserve_paragraphs: Try to keep paragraphs together when possible\n",
    "        \"\"\"\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.use_sentence_boundaries = use_sentence_boundaries\n",
    "        self.min_chunk_tokens = min_chunk_tokens\n",
    "        self.preserve_paragraphs = preserve_paragraphs\n",
    "        \n",
    "        # Validation\n",
    "        if overlap_tokens >= max_tokens:\n",
    "            raise ValueError(f\"Overlap ({overlap_tokens}) must be less than max_tokens ({max_tokens})\")\n",
    "        if min_chunk_tokens > max_tokens:\n",
    "            raise ValueError(f\"min_chunk_tokens ({min_chunk_tokens}) cannot exceed max_tokens ({max_tokens})\")\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize input text while preserving structure.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw input text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text with preserved paragraph boundaries\n",
    "        \"\"\"\n",
    "        # Remove image captions and metadata\n",
    "        text = re.sub(r'Enlarge this image.*?AP', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        text = re.sub(r'toggle caption.*?AP', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)  # Remove bracketed content\n",
    "        \n",
    "        # Normalize whitespace but preserve paragraph breaks\n",
    "        text = re.sub(r' +', ' ', text)  # Multiple spaces to single\n",
    "        text = re.sub(r'\\n\\n+', '\\n\\n', text)  # Multiple newlines to double\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def split_into_articles(self, document: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split multi-document cluster into individual articles.\n",
    "        \n",
    "        Args:\n",
    "            document: Raw document string\n",
    "            \n",
    "        Returns:\n",
    "            List of individual articles\n",
    "        \"\"\"\n",
    "        # Split by article separator\n",
    "        articles = [art.strip() for art in document.split(\"|||\") if len(art.strip()) > 0]\n",
    "        \n",
    "        if len(articles) == 0:\n",
    "            articles = [document.strip()]\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into sentences using NLTK's robust sentence tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            List of sentences\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sentences = sent_tokenize(text)\n",
    "        except:\n",
    "            # Fallback to simple splitting if NLTK fails\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        return [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "    \n",
    "    def split_into_paragraphs(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into paragraphs.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            List of paragraphs\n",
    "        \"\"\"\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 0]\n",
    "        \n",
    "        # If no paragraph breaks found, treat as single paragraph\n",
    "        if len(paragraphs) == 0:\n",
    "            paragraphs = [text.strip()]\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    def get_token_count(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Get accurate token count for text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Number of tokens\n",
    "        \"\"\"\n",
    "        return len(self.tokenizer.tokenize(text))\n",
    "    \n",
    "    def find_optimal_overlap_sentences(self, \n",
    "                                       previous_sentences: List[str], \n",
    "                                       target_overlap_tokens: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Find the optimal number of sentences for overlap to approximate target tokens.\n",
    "        \n",
    "        Args:\n",
    "            previous_sentences: Sentences from previous chunk\n",
    "            target_overlap_tokens: Target number of overlap tokens\n",
    "            \n",
    "        Returns:\n",
    "            List of sentences for overlap\n",
    "        \"\"\"\n",
    "        if not previous_sentences:\n",
    "            return []\n",
    "        \n",
    "        # Start from the end and work backwards\n",
    "        overlap_sentences = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sent in reversed(previous_sentences):\n",
    "            sent_tokens = self.get_token_count(sent)\n",
    "            \n",
    "            # Check if adding this sentence would exceed target significantly\n",
    "            if current_tokens > 0 and current_tokens + sent_tokens > target_overlap_tokens * 1.5:\n",
    "                break\n",
    "            \n",
    "            overlap_sentences.insert(0, sent)\n",
    "            current_tokens += sent_tokens\n",
    "            \n",
    "            # Stop if we've reached approximately the target\n",
    "            if current_tokens >= target_overlap_tokens * 0.8:\n",
    "                break\n",
    "        \n",
    "        return overlap_sentences\n",
    "    \n",
    "    def chunk_with_sentence_boundaries(self, \n",
    "                                       sentences: List[str], \n",
    "                                       article_idx: int,\n",
    "                                       previous_chunk_sentences: Optional[List[str]] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create chunks that respect sentence boundaries.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of sentences to chunk\n",
    "            article_idx: Index of source article\n",
    "            previous_chunk_sentences: Sentences from previous chunk for overlap\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        current_sentences = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Add overlap from previous chunk if exists\n",
    "        if previous_chunk_sentences:\n",
    "            overlap_sentences = self.find_optimal_overlap_sentences(\n",
    "                previous_chunk_sentences, \n",
    "                self.overlap_tokens\n",
    "            )\n",
    "            current_sentences.extend(overlap_sentences)\n",
    "            current_tokens = sum(self.get_token_count(s) for s in overlap_sentences)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent_tokens = self.get_token_count(sent)\n",
    "            \n",
    "            # Check if adding this sentence would exceed max_tokens\n",
    "            if current_tokens + sent_tokens > self.max_tokens:\n",
    "                # Only create chunk if it meets minimum size\n",
    "                if current_tokens >= self.min_chunk_tokens or len(chunks) == 0:\n",
    "                    chunk_text = ' '.join(current_sentences)\n",
    "                    chunks.append({\n",
    "                        'chunk_id': len(chunks),\n",
    "                        'text': chunk_text,\n",
    "                        'sentences': current_sentences.copy(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'sentence_count': len(current_sentences),\n",
    "                        'article_indices': [article_idx],\n",
    "                        'has_overlap': len(chunks) > 0 or previous_chunk_sentences is not None,\n",
    "                        'overlap_token_count': sum(self.get_token_count(s) for s in overlap_sentences) if previous_chunk_sentences else 0\n",
    "                    })\n",
    "                    \n",
    "                    # Start new chunk with overlap from current chunk\n",
    "                    overlap_sentences = self.find_optimal_overlap_sentences(\n",
    "                        current_sentences,\n",
    "                        self.overlap_tokens\n",
    "                    )\n",
    "                    current_sentences = overlap_sentences + [sent]\n",
    "                    current_tokens = sum(self.get_token_count(s) for s in current_sentences)\n",
    "                else:\n",
    "                    # Chunk too small, add sentence anyway\n",
    "                    current_sentences.append(sent)\n",
    "                    current_tokens += sent_tokens\n",
    "            else:\n",
    "                # Add sentence to current chunk\n",
    "                current_sentences.append(sent)\n",
    "                current_tokens += sent_tokens\n",
    "        \n",
    "        # Add final chunk if exists\n",
    "        if current_sentences and (current_tokens >= self.min_chunk_tokens or len(chunks) == 0):\n",
    "            chunk_text = ' '.join(current_sentences)\n",
    "            overlap_count = sum(self.get_token_count(s) for s in overlap_sentences) if len(chunks) > 0 or previous_chunk_sentences else 0\n",
    "            chunks.append({\n",
    "                'chunk_id': len(chunks),\n",
    "                'text': chunk_text,\n",
    "                'sentences': current_sentences.copy(),\n",
    "                'token_count': current_tokens,\n",
    "                'sentence_count': len(current_sentences),\n",
    "                'article_indices': [article_idx],\n",
    "                'has_overlap': len(chunks) > 0 or previous_chunk_sentences is not None,\n",
    "                'overlap_token_count': overlap_count\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_document(self, document: str) -> List[Dict[str, any]]:\n",
    "        \"\"\"\n",
    "        Main chunking function with semantic boundary preservation.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Clean and split document into articles\n",
    "        2. For each article, split into sentences\n",
    "        3. Create chunks respecting sentence boundaries\n",
    "        4. Maintain adaptive overlap between chunks\n",
    "        5. Track comprehensive metadata\n",
    "        \n",
    "        Args:\n",
    "            document: Raw document text (single or multi-document)\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with metadata\n",
    "        \"\"\"\n",
    "        # Step 1: Clean text\n",
    "        cleaned_doc = self.clean_text(document)\n",
    "        \n",
    "        # Step 2: Split into articles\n",
    "        articles = self.split_into_articles(cleaned_doc)\n",
    "        \n",
    "        # Step 3: Process articles into chunks\n",
    "        all_chunks = []\n",
    "        previous_chunk_sentences = None\n",
    "        \n",
    "        for article_idx, article in enumerate(articles):\n",
    "            if self.preserve_paragraphs:\n",
    "                # Split into paragraphs first, then sentences\n",
    "                paragraphs = self.split_into_paragraphs(article)\n",
    "                article_sentences = []\n",
    "                for para in paragraphs:\n",
    "                    para_sentences = self.split_into_sentences(para)\n",
    "                    article_sentences.extend(para_sentences)\n",
    "            else:\n",
    "                # Direct sentence splitting\n",
    "                article_sentences = self.split_into_sentences(article)\n",
    "            \n",
    "            # Create chunks for this article\n",
    "            if self.use_sentence_boundaries:\n",
    "                article_chunks = self.chunk_with_sentence_boundaries(\n",
    "                    article_sentences,\n",
    "                    article_idx,\n",
    "                    previous_chunk_sentences\n",
    "                )\n",
    "            else:\n",
    "                # Fallback to token-based chunking (original method)\n",
    "                article_chunks = self._chunk_by_tokens(article, article_idx)\n",
    "            \n",
    "            # Update chunk IDs to be sequential across all articles\n",
    "            for chunk in article_chunks:\n",
    "                chunk['chunk_id'] = len(all_chunks)\n",
    "                all_chunks.append(chunk)\n",
    "            \n",
    "            # Update previous chunk sentences for next article\n",
    "            if article_chunks:\n",
    "                previous_chunk_sentences = article_chunks[-1].get('sentences', [])\n",
    "        \n",
    "        return all_chunks\n",
    "    \n",
    "    def _chunk_by_tokens(self, text: str, article_idx: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fallback token-based chunking (original method).\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            article_idx: Article index\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        chunks = []\n",
    "        \n",
    "        start_idx = 0\n",
    "        while start_idx < len(tokens):\n",
    "            end_idx = min(start_idx + self.max_tokens, len(tokens))\n",
    "            chunk_tokens = tokens[start_idx:end_idx]\n",
    "            chunk_text = self.tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "            \n",
    "            chunks.append({\n",
    "                'chunk_id': len(chunks),\n",
    "                'text': chunk_text,\n",
    "                'token_count': len(chunk_tokens),\n",
    "                'article_indices': [article_idx],\n",
    "                'has_overlap': len(chunks) > 0,\n",
    "                'overlap_token_count': self.overlap_tokens if len(chunks) > 0 else 0\n",
    "            })\n",
    "            \n",
    "            # Move start index forward (with overlap)\n",
    "            start_idx = end_idx - self.overlap_tokens if end_idx < len(tokens) else end_idx\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def validate_chunks(self, chunks: List[Dict[str, any]]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate chunks and return detailed diagnostics.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunk dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (is_valid, list of warning messages)\n",
    "        \"\"\"\n",
    "        warnings = []\n",
    "        is_valid = True\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Check token count\n",
    "            if chunk['token_count'] > self.max_tokens:\n",
    "                warnings.append(f\"Chunk {i}: Exceeds max_tokens ({chunk['token_count']} > {self.max_tokens})\")\n",
    "                is_valid = False\n",
    "            \n",
    "            # Check minimum size (except for last chunk)\n",
    "            if chunk['token_count'] < self.min_chunk_tokens and i < len(chunks) - 1:\n",
    "                warnings.append(f\"Chunk {i}: Below min_chunk_tokens ({chunk['token_count']} < {self.min_chunk_tokens})\")\n",
    "            \n",
    "            # Verify actual token count\n",
    "            actual_tokens = self.get_token_count(chunk['text'])\n",
    "            if abs(actual_tokens - chunk['token_count']) > 5:\n",
    "                warnings.append(f\"Chunk {i}: Token count mismatch (reported={chunk['token_count']}, actual={actual_tokens})\")\n",
    "            \n",
    "            # Check for sentence splitting if enabled\n",
    "            if self.use_sentence_boundaries and 'sentences' in chunk:\n",
    "                reconstructed = ' '.join(chunk['sentences'])\n",
    "                if reconstructed != chunk['text']:\n",
    "                    warnings.append(f\"Chunk {i}: Sentence reconstruction mismatch\")\n",
    "        \n",
    "        return is_valid, warnings\n",
    "    \n",
    "    def get_summary_statistics(self, chunks: List[Dict[str, any]]) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Comprehensive statistics about chunking process.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunk dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detailed statistics\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return {'num_chunks': 0}\n",
    "        \n",
    "        token_counts = [c['token_count'] for c in chunks]\n",
    "        overlap_counts = [c.get('overlap_token_count', 0) for c in chunks if c.get('has_overlap', False)]\n",
    "        sentence_counts = [c.get('sentence_count', 0) for c in chunks if 'sentence_count' in c]\n",
    "        \n",
    "        stats = {\n",
    "            'num_chunks': len(chunks),\n",
    "            'total_tokens': sum(token_counts),\n",
    "            'total_overlap_tokens': sum(overlap_counts),\n",
    "            'net_tokens': sum(token_counts) - sum(overlap_counts),\n",
    "            'avg_tokens_per_chunk': np.mean(token_counts),\n",
    "            'std_tokens_per_chunk': np.std(token_counts),\n",
    "            'min_tokens': min(token_counts),\n",
    "            'max_tokens': max(token_counts),\n",
    "            'chunks_with_overlap': sum(1 for c in chunks if c.get('has_overlap', False)),\n",
    "            'avg_overlap_tokens': np.mean(overlap_counts) if overlap_counts else 0,\n",
    "            'token_efficiency': (sum(token_counts) - sum(overlap_counts)) / sum(token_counts) * 100 if sum(token_counts) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        if sentence_counts:\n",
    "            stats.update({\n",
    "                'total_sentences': sum(sentence_counts),\n",
    "                'avg_sentences_per_chunk': np.mean(sentence_counts),\n",
    "                'min_sentences': min(sentence_counts),\n",
    "                'max_sentences': max(sentence_counts)\n",
    "            })\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def export_chunks_for_analysis(self, chunks: List[Dict[str, any]]) -> str:\n",
    "        \"\"\"\n",
    "        Export chunks in a format suitable for ablation studies and analysis.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunk dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            JSON-formatted string\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        export_data = {\n",
    "            'chunking_config': {\n",
    "                'max_tokens': self.max_tokens,\n",
    "                'overlap_tokens': self.overlap_tokens,\n",
    "                'use_sentence_boundaries': self.use_sentence_boundaries,\n",
    "                'min_chunk_tokens': self.min_chunk_tokens,\n",
    "                'preserve_paragraphs': self.preserve_paragraphs\n",
    "            },\n",
    "            'statistics': self.get_summary_statistics(chunks),\n",
    "            'chunks': chunks\n",
    "        }\n",
    "        \n",
    "        return json.dumps(export_data, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load Multi-News dataset\n",
    "multi_news = load_dataset(\"Awesome075/multi_news_parquet\")\n",
    "test_example = multi_news[\"test\"][0]\n",
    "\n",
    "# Initialize semantic chunker\n",
    "chunker = SemanticDocumentChunker(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    max_tokens=1024,\n",
    "    overlap_tokens=128,\n",
    "    use_sentence_boundaries=True,  # Key innovation!\n",
    "    min_chunk_tokens=256,\n",
    "    preserve_paragraphs=True\n",
    ")\n",
    "\n",
    "# Process document\n",
    "chunks = chunker.chunk_document(test_example[\"document\"])\n",
    "\n",
    "# Validate with diagnostics\n",
    "is_valid, warnings = chunker.validate_chunks(chunks)\n",
    "print(f\"Validation: {is_valid}\")\n",
    "if warnings:\n",
    "    for w in warnings:\n",
    "        print(f\"  ⚠ {w}\")\n",
    "\n",
    "# Comprehensive statistics\n",
    "stats = chunker.get_summary_statistics(chunks)\n",
    "print(\"\\nStatistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Detailed chunk analysis\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}/{1024}\")\n",
    "    print(f\"  Sentences: {chunk.get('sentence_count', 'N/A')}\")\n",
    "    print(f\"  Overlap tokens: {chunk.get('overlap_token_count', 0)}\")\n",
    "    print(f\"  Preview: {chunk['text'][:200]}...\")\n",
    "\n",
    "# Export for reproducibility\n",
    "export_json = chunker.export_chunks_for_analysis(chunks)\n",
    "# Save to file if needed\n",
    "# with open('chunks_analysis.json', 'w') as f:\n",
    "#     f.write(export_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593729c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "\n",
    "@dataclass\n",
    "class SummarizationConfig:\n",
    "    \"\"\"Configuration for hierarchical summarization pipeline.\"\"\"\n",
    "    \n",
    "    # Chunk-level summarization parameters\n",
    "    chunk_max_length: int = 256  # Max length for chunk summaries\n",
    "    chunk_min_length: int = 64   # Min length for chunk summaries\n",
    "    chunk_num_beams: int = 4     # Beam search width\n",
    "    chunk_length_penalty: float = 1.0  # Length penalty factor\n",
    "    chunk_no_repeat_ngram_size: int = 3  # Prevent n-gram repetition\n",
    "    \n",
    "    # Cluster-level summarization parameters  \n",
    "    cluster_max_length: int = 256  # Max length for final summary\n",
    "    cluster_min_length: int = 100  # Min length for final summary\n",
    "    cluster_num_beams: int = 6     # More beams for final summary\n",
    "    cluster_length_penalty: float = 1.2  # Encourage longer summaries\n",
    "    cluster_no_repeat_ngram_size: int = 3\n",
    "    \n",
    "    # Advanced generation parameters\n",
    "    temperature: float = 1.0  # Sampling temperature\n",
    "    top_k: int = 50  # Top-k sampling\n",
    "    top_p: float = 0.95  # Nucleus sampling\n",
    "    do_sample: bool = False  # Use deterministic beam search\n",
    "    early_stopping: bool = True  # Stop when all beams finish\n",
    "    \n",
    "    # Hierarchical strategy\n",
    "    use_extractive_augmentation: bool = False  # Add extractive sentences\n",
    "    use_query_focused: bool = False  # Query-focused summarization\n",
    "    fusion_strategy: str = \"concatenate\"  # concatenate, attention-weighted, or iterative\n",
    "\n",
    "\n",
    "class HierarchicalSummarizer:\n",
    "    \"\"\"\n",
    "    State-of-the-art hierarchical multi-document summarization system.\n",
    "    \n",
    "    Implements a two-stage approach:\n",
    "    1. Chunk-level: Summarize each chunk independently with context preservation\n",
    "    2. Cluster-level: Fuse chunk summaries into coherent final summary\n",
    "    \n",
    "    Key innovations:\n",
    "    - Adaptive summary length based on chunk importance\n",
    "    - Context-aware prompting for chunk summaries\n",
    "    - Multi-strategy fusion (concatenation, iterative refinement)\n",
    "    - Diversity-promoting beam search\n",
    "    - Redundancy reduction across hierarchy levels\n",
    "    \n",
    "    Reference: FactSum (Gupta, 2025)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"google/pegasus-multi_news\",\n",
    "                 config: Optional[SummarizationConfig] = None,\n",
    "                 device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize hierarchical summarizer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model identifier\n",
    "            config: Summarization configuration\n",
    "            device: Device to use (cuda/cpu/mps)\n",
    "        \"\"\"\n",
    "        self.config = config or SummarizationConfig()\n",
    "        \n",
    "        # Auto-detect device\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = \"cuda\"\n",
    "            elif torch.backends.mps.is_available():\n",
    "                self.device = \"mps\"\n",
    "            else:\n",
    "                self.device = \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Loading model on device: {self.device}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.stats = {\n",
    "            'chunks_processed': 0,\n",
    "            'total_input_tokens': 0,\n",
    "            'total_output_tokens': 0,\n",
    "            'chunk_summaries_generated': 0,\n",
    "            'cluster_summaries_generated': 0\n",
    "        }\n",
    "    \n",
    "    def _prepare_chunk_input(self, \n",
    "                            chunk: Dict[str, any], \n",
    "                            chunk_idx: int,\n",
    "                            total_chunks: int) -> str:\n",
    "        \"\"\"\n",
    "        Prepare chunk text with optional context-aware prompting.\n",
    "        \n",
    "        Args:\n",
    "            chunk: Chunk dictionary from semantic chunker\n",
    "            chunk_idx: Current chunk index\n",
    "            total_chunks: Total number of chunks\n",
    "            \n",
    "        Returns:\n",
    "            Prepared input text\n",
    "        \"\"\"\n",
    "        text = chunk['text']\n",
    "        \n",
    "        # Optional: Add positional encoding through prompting\n",
    "        # This helps model understand document structure\n",
    "        if self.config.use_query_focused:\n",
    "            if chunk_idx == 0:\n",
    "                # First chunk - focus on main topic/event\n",
    "                prefix = \"Summarize the main event and key details: \"\n",
    "            elif chunk_idx == total_chunks - 1:\n",
    "                # Last chunk - focus on outcomes/implications\n",
    "                prefix = \"Summarize the outcomes and implications: \"\n",
    "            else:\n",
    "                # Middle chunks - focus on developments\n",
    "                prefix = \"Summarize the key developments: \"\n",
    "            \n",
    "            text = prefix + text\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def summarize_chunk(self, \n",
    "                       chunk: Dict[str, any],\n",
    "                       chunk_idx: int = 0,\n",
    "                       total_chunks: int = 1) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Generate summary for a single chunk.\n",
    "        \n",
    "        Args:\n",
    "            chunk: Chunk dictionary\n",
    "            chunk_idx: Index of this chunk\n",
    "            total_chunks: Total number of chunks\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with summary and metadata\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        input_text = self._prepare_chunk_input(chunk, chunk_idx, total_chunks)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Track input tokens\n",
    "        input_token_count = inputs['input_ids'].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "        self.stats['total_input_tokens'] += input_token_count\n",
    "        \n",
    "        # Generate summary\n",
    "        with torch.no_grad():\n",
    "            summary_ids = self.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=self.config.chunk_max_length,\n",
    "                min_length=self.config.chunk_min_length,\n",
    "                num_beams=self.config.chunk_num_beams,\n",
    "                length_penalty=self.config.chunk_length_penalty,\n",
    "                no_repeat_ngram_size=self.config.chunk_no_repeat_ngram_size,\n",
    "                early_stopping=self.config.early_stopping,\n",
    "                do_sample=self.config.do_sample,\n",
    "                temperature=self.config.temperature if self.config.do_sample else 1.0,\n",
    "                top_k=self.config.top_k if self.config.do_sample else 50,\n",
    "                top_p=self.config.top_p if self.config.do_sample else 1.0,\n",
    "            )\n",
    "        \n",
    "        # Decode summary\n",
    "        summary_text = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        output_token_count = len(self.tokenizer.tokenize(summary_text))\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['chunks_processed'] += 1\n",
    "        self.stats['chunk_summaries_generated'] += 1\n",
    "        self.stats['total_output_tokens'] += output_token_count\n",
    "        \n",
    "        return {\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'summary': summary_text,\n",
    "            'input_tokens': input_token_count,\n",
    "            'output_tokens': output_token_count,\n",
    "            'compression_ratio': input_token_count / output_token_count if output_token_count > 0 else 0,\n",
    "            'original_chunk': chunk\n",
    "        }\n",
    "    \n",
    "    def _fuse_summaries_concatenate(self, chunk_summaries: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Simple concatenation fusion strategy.\n",
    "        \n",
    "        Args:\n",
    "            chunk_summaries: List of chunk summary dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Concatenated text\n",
    "        \"\"\"\n",
    "        summaries_text = [cs['summary'] for cs in chunk_summaries]\n",
    "        return \" \".join(summaries_text)\n",
    "    \n",
    "    def _fuse_summaries_iterative(self, chunk_summaries: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Iterative refinement fusion strategy.\n",
    "        Progressively merge summaries in pairs.\n",
    "        \n",
    "        Args:\n",
    "            chunk_summaries: List of chunk summary dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Fused summary text\n",
    "        \"\"\"\n",
    "        if len(chunk_summaries) == 1:\n",
    "            return chunk_summaries[0]['summary']\n",
    "        \n",
    "        summaries = [cs['summary'] for cs in chunk_summaries]\n",
    "        \n",
    "        # Iteratively merge pairs\n",
    "        while len(summaries) > 1:\n",
    "            merged = []\n",
    "            for i in range(0, len(summaries), 2):\n",
    "                if i + 1 < len(summaries):\n",
    "                    # Merge two summaries\n",
    "                    pair_text = summaries[i] + \" \" + summaries[i + 1]\n",
    "                    \n",
    "                    # Summarize the pair\n",
    "                    inputs = self.tokenizer(\n",
    "                        pair_text,\n",
    "                        max_length=1024,\n",
    "                        truncation=True,\n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        summary_ids = self.model.generate(\n",
    "                            inputs['input_ids'],\n",
    "                            max_length=self.config.cluster_max_length // 2,\n",
    "                            num_beams=4,\n",
    "                            early_stopping=True\n",
    "                        )\n",
    "                    \n",
    "                    merged_summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "                    merged.append(merged_summary)\n",
    "                else:\n",
    "                    # Odd one out\n",
    "                    merged.append(summaries[i])\n",
    "            \n",
    "            summaries = merged\n",
    "        \n",
    "        return summaries[0]\n",
    "    \n",
    "    def generate_cluster_summary(self, \n",
    "                                 chunk_summaries: List[Dict],\n",
    "                                 fusion_strategy: Optional[str] = None) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Generate final cluster-level summary from chunk summaries.\n",
    "        \n",
    "        Args:\n",
    "            chunk_summaries: List of chunk summary dictionaries\n",
    "            fusion_strategy: Strategy to use (overrides config)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with final summary and metadata\n",
    "        \"\"\"\n",
    "        strategy = fusion_strategy or self.config.fusion_strategy\n",
    "        \n",
    "        # Fuse chunk summaries based on strategy\n",
    "        if strategy == \"concatenate\":\n",
    "            fused_text = self._fuse_summaries_concatenate(chunk_summaries)\n",
    "        elif strategy == \"iterative\":\n",
    "            fused_text = self._fuse_summaries_iterative(chunk_summaries)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion strategy: {strategy}\")\n",
    "        \n",
    "        # Check if fused text fits in one pass\n",
    "        fused_tokens = len(self.tokenizer.tokenize(fused_text))\n",
    "        \n",
    "        if fused_tokens <= 1024:\n",
    "            # Can process in single pass\n",
    "            inputs = self.tokenizer(\n",
    "                fused_text,\n",
    "                max_length=1024,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            input_token_count = inputs['input_ids'].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "            \n",
    "            # Generate final summary\n",
    "            with torch.no_grad():\n",
    "                summary_ids = self.model.generate(\n",
    "                    inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_length=self.config.cluster_max_length,\n",
    "                    min_length=self.config.cluster_min_length,\n",
    "                    num_beams=self.config.cluster_num_beams,\n",
    "                    length_penalty=self.config.cluster_length_penalty,\n",
    "                    no_repeat_ngram_size=self.config.cluster_no_repeat_ngram_size,\n",
    "                    early_stopping=self.config.early_stopping,\n",
    "                    diversity_penalty=0.5,  # Encourage diverse beam outputs\n",
    "                )\n",
    "            \n",
    "            final_summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "        else:\n",
    "            # Fused text too long - use iterative strategy\n",
    "            warnings.warn(f\"Fused summaries ({fused_tokens} tokens) exceed limit. Using iterative refinement.\")\n",
    "            final_summary = self._fuse_summaries_iterative(chunk_summaries)\n",
    "        \n",
    "        output_token_count = len(self.tokenizer.tokenize(final_summary))\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['cluster_summaries_generated'] += 1\n",
    "        self.stats['total_output_tokens'] += output_token_count\n",
    "        \n",
    "        return {\n",
    "            'summary': final_summary,\n",
    "            'fusion_strategy': strategy,\n",
    "            'num_chunk_summaries': len(chunk_summaries),\n",
    "            'fused_input_tokens': fused_tokens,\n",
    "            'output_tokens': output_token_count,\n",
    "            'compression_ratio': fused_tokens / output_token_count if output_token_count > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def summarize_document(self, \n",
    "                          chunks: List[Dict[str, any]],\n",
    "                          return_chunk_summaries: bool = True) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Complete hierarchical summarization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunk dictionaries from semantic chunker\n",
    "            return_chunk_summaries: Whether to return intermediate summaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with final summary and all metadata\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"HIERARCHICAL SUMMARIZATION PIPELINE\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Stage 1: Chunk-level summarization\n",
    "        print(f\"Stage 1: Chunk-level Summarization\")\n",
    "        print(f\"-\" * 80)\n",
    "        \n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"  Processing chunk {i+1}/{len(chunks)} ({chunk['token_count']} tokens)...\", end=\" \")\n",
    "            \n",
    "            chunk_summary = self.summarize_chunk(chunk, i, len(chunks))\n",
    "            chunk_summaries.append(chunk_summary)\n",
    "            \n",
    "            print(f\"✓ ({chunk_summary['output_tokens']} tokens, \"\n",
    "                  f\"{chunk_summary['compression_ratio']:.2f}x compression)\")\n",
    "        \n",
    "        print(f\"\\n  Total chunk summaries: {len(chunk_summaries)}\")\n",
    "        total_chunk_tokens = sum(cs['output_tokens'] for cs in chunk_summaries)\n",
    "        print(f\"  Combined length: {total_chunk_tokens} tokens\\n\")\n",
    "        \n",
    "        # Stage 2: Cluster-level summarization\n",
    "        print(f\"Stage 2: Cluster-level Summarization\")\n",
    "        print(f\"-\" * 80)\n",
    "        print(f\"  Fusing {len(chunk_summaries)} chunk summaries using '{self.config.fusion_strategy}' strategy...\")\n",
    "        \n",
    "        cluster_summary = self.generate_cluster_summary(chunk_summaries)\n",
    "        \n",
    "        print(f\"  ✓ Final summary: {cluster_summary['output_tokens']} tokens\")\n",
    "        print(f\"  Compression: {cluster_summary['compression_ratio']:.2f}x\\n\")\n",
    "        \n",
    "        # Compile results\n",
    "        result = {\n",
    "            'final_summary': cluster_summary['summary'],\n",
    "            'cluster_metadata': cluster_summary,\n",
    "            'num_chunks': len(chunks),\n",
    "            'total_compression_ratio': self.stats['total_input_tokens'] / cluster_summary['output_tokens'] \n",
    "                                      if cluster_summary['output_tokens'] > 0 else 0,\n",
    "            'statistics': self.stats.copy()\n",
    "        }\n",
    "        \n",
    "        if return_chunk_summaries:\n",
    "            result['chunk_summaries'] = chunk_summaries\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def reset_statistics(self):\n",
    "        \"\"\"Reset internal statistics counters.\"\"\"\n",
    "        self.stats = {\n",
    "            'chunks_processed': 0,\n",
    "            'total_input_tokens': 0,\n",
    "            'total_output_tokens': 0,\n",
    "            'chunk_summaries_generated': 0,\n",
    "            'cluster_summaries_generated': 0\n",
    "        }\n",
    "    \n",
    "    def get_statistics_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Get formatted statistics summary.\n",
    "        \n",
    "        Returns:\n",
    "            Formatted statistics string\n",
    "        \"\"\"\n",
    "        s = self.stats\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "Summarization Statistics:\n",
    "{'='*80}\n",
    "Chunks processed: {s['chunks_processed']}\n",
    "Chunk summaries generated: {s['chunk_summaries_generated']}\n",
    "Cluster summaries generated: {s['cluster_summaries_generated']}\n",
    "\n",
    "Token Analysis:\n",
    "  Total input tokens: {s['total_input_tokens']:,}\n",
    "  Total output tokens: {s['total_output_tokens']:,}\n",
    "  Overall compression ratio: {s['total_input_tokens']/s['total_output_tokens']:.2f}x\n",
    "  \n",
    "Efficiency:\n",
    "  Avg tokens per chunk: {s['total_input_tokens']/s['chunks_processed']:.0f}\n",
    "  Avg output per summary: {s['total_output_tokens']/(s['chunk_summaries_generated'] + s['cluster_summaries_generated']):.0f}\n",
    "\"\"\"\n",
    "        return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and demonstration code\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"State-of-the-Art Hierarchical Summarization System\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nFor Jupyter notebook testing, use:\\n\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset and prepare chunks (using semantic chunker from previous step)\n",
    "multi_news = load_dataset(\"Awesome075/multi_news_parquet\")\n",
    "test_example = multi_news[\"test\"][0]\n",
    "\n",
    "# Initialize semantic chunker\n",
    "from semantic_chunker import SemanticDocumentChunker  # Your previous code\n",
    "\n",
    "chunker = SemanticDocumentChunker(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    max_tokens=1024,\n",
    "    overlap_tokens=128,\n",
    "    use_sentence_boundaries=True\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_document(test_example[\"document\"])\n",
    "\n",
    "# Initialize hierarchical summarizer\n",
    "config = SummarizationConfig(\n",
    "    chunk_max_length=256,\n",
    "    chunk_min_length=64,\n",
    "    cluster_max_length=256,\n",
    "    cluster_min_length=100,\n",
    "    chunk_num_beams=4,\n",
    "    cluster_num_beams=6,\n",
    "    fusion_strategy=\"concatenate\"  # or \"iterative\"\n",
    ")\n",
    "\n",
    "summarizer = HierarchicalSummarizer(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run hierarchical summarization\n",
    "result = summarizer.summarize_document(chunks, return_chunk_summaries=True)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL CLUSTER SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_summary'])\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"HUMAN REFERENCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(test_example[\"summary\"])\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"CHUNK-LEVEL SUMMARIES\")\n",
    "print(\"=\"*80)\n",
    "for i, cs in enumerate(result['chunk_summaries']):\n",
    "    print(f\"\\\\nChunk {i} Summary ({cs['output_tokens']} tokens):\")\n",
    "    print(cs['summary'])\n",
    "\n",
    "# Statistics\n",
    "print(summarizer.get_statistics_summary())\n",
    "\n",
    "# Optional: Compare different fusion strategies\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"COMPARING FUSION STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for strategy in [\"concatenate\", \"iterative\"]:\n",
    "    summarizer.reset_statistics()\n",
    "    config.fusion_strategy = strategy\n",
    "    \n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    \n",
    "    print(f\"\\\\nStrategy: {strategy}\")\n",
    "    print(f\"  Summary length: {result['cluster_metadata']['output_tokens']} tokens\")\n",
    "    print(f\"  Compression: {result['total_compression_ratio']:.2f}x\")\n",
    "    print(f\"  Preview: {result['final_summary'][:200]}...\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset and prepare chunks (using semantic chunker from previous step)\n",
    "multi_news = load_dataset(\"Awesome075/multi_news_parquet\")\n",
    "test_example = multi_news[\"test\"][0]\n",
    "\n",
    "# Initialize semantic chunker\n",
    "# Your previous code\n",
    "\n",
    "chunker = SemanticDocumentChunker(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    max_tokens=1024,\n",
    "    overlap_tokens=128,\n",
    "    use_sentence_boundaries=True\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk_document(test_example[\"document\"])\n",
    "\n",
    "# Initialize hierarchical summarizer\n",
    "config = SummarizationConfig(\n",
    "    chunk_max_length=256,\n",
    "    chunk_min_length=64,\n",
    "    cluster_max_length=256,\n",
    "    cluster_min_length=100,\n",
    "    chunk_num_beams=4,\n",
    "    cluster_num_beams=6,\n",
    "    fusion_strategy=\"concatenate\"  # or \"iterative\"\n",
    ")\n",
    "\n",
    "summarizer = HierarchicalSummarizer(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Run hierarchical summarization\n",
    "result = summarizer.summarize_document(chunks, return_chunk_summaries=True)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL CLUSTER SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_summary'])\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"HUMAN REFERENCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(test_example[\"summary\"])\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"CHUNK-LEVEL SUMMARIES\")\n",
    "print(\"=\"*80)\n",
    "for i, cs in enumerate(result['chunk_summaries']):\n",
    "    print(f\"\\\\nChunk {i} Summary ({cs['output_tokens']} tokens):\")\n",
    "    print(cs['summary'])\n",
    "\n",
    "# Statistics\n",
    "print(summarizer.get_statistics_summary())\n",
    "\n",
    "# Optional: Compare different fusion strategies\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"COMPARING FUSION STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for strategy in [\"concatenate\", \"iterative\"]:\n",
    "    summarizer.reset_statistics()\n",
    "    config.fusion_strategy = strategy\n",
    "    \n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    \n",
    "    print(f\"\\\\nStrategy: {strategy}\")\n",
    "    print(f\"  Summary length: {result['cluster_metadata']['output_tokens']} tokens\")\n",
    "    print(f\"  Compression: {result['total_compression_ratio']:.2f}x\")\n",
    "    print(f\"  Preview: {result['final_summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FactSum Complete Testing Pipeline\n",
    "==================================\n",
    "This notebook cell tests the complete hierarchical summarization system\n",
    "with semantic chunking on the Multi-News dataset.\n",
    "\n",
    "Prerequisites: \n",
    "- SemanticDocumentChunker class loaded\n",
    "- HierarchicalSummarizer class loaded\n",
    "- Multi-News dataset loaded\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FactSum: Complete Testing Pipeline\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Initialize semantic chunker\n",
    "print(\"Step 1: Initializing Semantic Document Chunker...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "chunker = SemanticDocumentChunker(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    max_tokens=1024,\n",
    "    overlap_tokens=128,\n",
    "    use_sentence_boundaries=True,\n",
    "    min_chunk_tokens=256,\n",
    "    preserve_paragraphs=True\n",
    ")\n",
    "print(\"✓ Chunker initialized\\n\")\n",
    "\n",
    "# Initialize hierarchical summarizer\n",
    "print(\"Step 2: Initializing Hierarchical Summarizer...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "config = SummarizationConfig(\n",
    "    # Chunk-level parameters\n",
    "    chunk_max_length=256,\n",
    "    chunk_min_length=64,\n",
    "    chunk_num_beams=4,\n",
    "    chunk_length_penalty=1.0,\n",
    "    chunk_no_repeat_ngram_size=3,\n",
    "    \n",
    "    # Cluster-level parameters\n",
    "    cluster_max_length=256,\n",
    "    cluster_min_length=100,\n",
    "    cluster_num_beams=6,\n",
    "    cluster_length_penalty=1.2,\n",
    "    cluster_no_repeat_ngram_size=3,\n",
    "    \n",
    "    # Strategy\n",
    "    fusion_strategy=\"concatenate\",  # Try \"iterative\" later\n",
    "    early_stopping=True,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "summarizer = HierarchicalSummarizer(\n",
    "    model_name=\"google/pegasus-multi_news\",\n",
    "    config=config\n",
    ")\n",
    "print(\"✓ Summarizer initialized\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# SINGLE DOCUMENT TEST\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST 1: Single Document Summarization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select test example\n",
    "test_example = multi_news[\"test\"][0]\n",
    "\n",
    "print(f\"\\nDocument Info:\")\n",
    "print(f\"  Original length: {len(test_example['document'])} characters\")\n",
    "print(f\"  Reference summary length: {len(test_example['summary'])} characters\")\n",
    "\n",
    "# Step 1: Chunk the document\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STAGE 1: SEMANTIC CHUNKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "chunks = chunker.chunk_document(test_example[\"document\"])\n",
    "\n",
    "# Validate chunks\n",
    "is_valid, warnings = chunker.validate_chunks(chunks)\n",
    "print(f\"\\nValidation: {'✓ PASSED' if is_valid else '✗ FAILED'}\")\n",
    "\n",
    "if warnings:\n",
    "    print(\"Warnings:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  ⚠ {w}\")\n",
    "\n",
    "# Get chunking statistics\n",
    "chunk_stats = chunker.get_summary_statistics(chunks)\n",
    "print(f\"\\nChunking Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "for key, value in chunk_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Display chunk details\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHUNK DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}/{1024} ({chunk['token_count']/1024*100:.1f}% full)\")\n",
    "    print(f\"  Sentences: {chunk.get('sentence_count', 'N/A')}\")\n",
    "    print(f\"  Overlap tokens: {chunk.get('overlap_token_count', 0)}\")\n",
    "    print(f\"  Has overlap: {chunk.get('has_overlap', False)}\")\n",
    "    print(f\"  Article indices: {chunk.get('article_indices', [])}\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\")\n",
    "\n",
    "# Step 2: Hierarchical Summarization\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STAGE 2: HIERARCHICAL SUMMARIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reset statistics for this run\n",
    "summarizer.reset_statistics()\n",
    "\n",
    "# Run summarization pipeline\n",
    "result = summarizer.summarize_document(chunks, return_chunk_summaries=True)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARIZATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 Overview:\")\n",
    "print(f\"  Number of chunks: {result['num_chunks']}\")\n",
    "print(f\"  Total compression: {result['total_compression_ratio']:.2f}x\")\n",
    "print(f\"  Final summary length: {result['cluster_metadata']['output_tokens']} tokens\")\n",
    "\n",
    "# Display chunk summaries\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHUNK-LEVEL SUMMARIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, cs in enumerate(result['chunk_summaries']):\n",
    "    print(f\"\\n--- Chunk {i} Summary ---\")\n",
    "    print(f\"Length: {cs['output_tokens']} tokens (compression: {cs['compression_ratio']:.2f}x)\")\n",
    "    print(f\"Summary: {cs['summary']}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL CLUSTER SUMMARY (GENERATED)\")\n",
    "print(\"=\"*80)\n",
    "print(result['final_summary'])\n",
    "\n",
    "# Display human reference\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HUMAN REFERENCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(test_example[\"summary\"])\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\n{summarizer.get_statistics_summary()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FUSION STRATEGY COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: Fusion Strategy Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fusion_results = {}\n",
    "\n",
    "for strategy in [\"concatenate\", \"iterative\"]:\n",
    "    print(f\"\\nTesting strategy: {strategy.upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Update config\n",
    "    config.fusion_strategy = strategy\n",
    "    \n",
    "    # Reset statistics\n",
    "    summarizer.reset_statistics()\n",
    "    \n",
    "    # Re-run summarization\n",
    "    strategy_result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    \n",
    "    fusion_results[strategy] = {\n",
    "        'summary': strategy_result['final_summary'],\n",
    "        'tokens': strategy_result['cluster_metadata']['output_tokens'],\n",
    "        'compression': strategy_result['total_compression_ratio']\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Summary length: {fusion_results[strategy]['tokens']} tokens\")\n",
    "    print(f\"  ✓ Compression: {fusion_results[strategy]['compression']:.2f}x\")\n",
    "    print(f\"  Preview: {fusion_results[strategy]['summary'][:200]}...\")\n",
    "\n",
    "# Comparison table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FUSION STRATEGY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'Strategy': [],\n",
    "    'Summary Length (tokens)': [],\n",
    "    'Compression Ratio': [],\n",
    "    'Summary Preview': []\n",
    "}\n",
    "\n",
    "for strategy, data in fusion_results.items():\n",
    "    comparison_data['Strategy'].append(strategy)\n",
    "    comparison_data['Summary Length (tokens)'].append(data['tokens'])\n",
    "    comparison_data['Compression Ratio'].append(f\"{data['compression']:.2f}x\")\n",
    "    comparison_data['Summary Preview'].append(data['summary'][:100] + \"...\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-DOCUMENT TEST (Multiple Examples)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: Multi-Document Batch Processing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on first 3 examples from test set\n",
    "num_examples = 3\n",
    "batch_results = []\n",
    "\n",
    "print(f\"\\nProcessing {num_examples} examples from test set...\")\n",
    "\n",
    "for idx in range(num_examples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing Example {idx + 1}/{num_examples}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    example = multi_news[\"test\"][idx]\n",
    "    \n",
    "    # Chunk document\n",
    "    doc_chunks = chunker.chunk_document(example[\"document\"])\n",
    "    \n",
    "    # Summarize\n",
    "    summarizer.reset_statistics()\n",
    "    doc_result = summarizer.summarize_document(doc_chunks, return_chunk_summaries=False)\n",
    "    \n",
    "    batch_results.append({\n",
    "        'example_id': idx,\n",
    "        'num_chunks': doc_result['num_chunks'],\n",
    "        'generated_summary': doc_result['final_summary'],\n",
    "        'reference_summary': example['summary'],\n",
    "        'compression_ratio': doc_result['total_compression_ratio'],\n",
    "        'output_tokens': doc_result['cluster_metadata']['output_tokens']\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✓ Chunks: {doc_result['num_chunks']}\")\n",
    "    print(f\"  ✓ Compression: {doc_result['total_compression_ratio']:.2f}x\")\n",
    "    print(f\"  ✓ Output: {doc_result['cluster_metadata']['output_tokens']} tokens\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BATCH PROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_df = pd.DataFrame(batch_results)\n",
    "print(f\"\\nAverage compression ratio: {batch_df['compression_ratio'].mean():.2f}x\")\n",
    "print(f\"Average output length: {batch_df['output_tokens'].mean():.0f} tokens\")\n",
    "print(f\"Average chunks per document: {batch_df['num_chunks'].mean():.1f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: Visualization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Compression ratios\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar(range(len(batch_results)), [r['compression_ratio'] for r in batch_results])\n",
    "ax1.set_xlabel('Example ID')\n",
    "ax1.set_ylabel('Compression Ratio')\n",
    "ax1.set_title('Compression Ratios Across Examples')\n",
    "ax1.axhline(y=batch_df['compression_ratio'].mean(), color='r', linestyle='--', label='Average')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Output token lengths\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(range(len(batch_results)), [r['output_tokens'] for r in batch_results], color='green')\n",
    "ax2.set_xlabel('Example ID')\n",
    "ax2.set_ylabel('Output Tokens')\n",
    "ax2.set_title('Summary Lengths')\n",
    "ax2.axhline(y=batch_df['output_tokens'].mean(), color='r', linestyle='--', label='Average')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Chunks per document\n",
    "ax3 = axes[1, 0]\n",
    "ax3.bar(range(len(batch_results)), [r['num_chunks'] for r in batch_results], color='orange')\n",
    "ax3.set_xlabel('Example ID')\n",
    "ax3.set_ylabel('Number of Chunks')\n",
    "ax3.set_title('Chunks per Document')\n",
    "\n",
    "# Plot 4: Fusion strategy comparison\n",
    "ax4 = axes[1, 1]\n",
    "strategies = list(fusion_results.keys())\n",
    "tokens = [fusion_results[s]['tokens'] for s in strategies]\n",
    "ax4.bar(strategies, tokens, color=['blue', 'purple'])\n",
    "ax4.set_xlabel('Fusion Strategy')\n",
    "ax4.set_ylabel('Summary Length (tokens)')\n",
    "ax4.set_title('Fusion Strategy Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualizations generated\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n✅ All tests passed successfully!\")\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  • Semantic chunking: {chunk_stats['token_efficiency']:.2f}% token efficiency\")\n",
    "print(f\"  • Average compression: {batch_df['compression_ratio'].mean():.2f}x\")\n",
    "print(f\"  • Average summary length: {batch_df['output_tokens'].mean():.0f} tokens\")\n",
    "print(f\"  • Average chunks per document: {batch_df['num_chunks'].mean():.1f}\")\n",
    "print(f\"  • Best fusion strategy: {max(fusion_results.items(), key=lambda x: x[1]['compression'])[0]}\")\n",
    "\n",
    "print(f\"\\n🎯 System ready for evaluation!\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Run ROUGE evaluation on batch_results\")\n",
    "print(f\"  2. Implement fact verification module\")\n",
    "print(f\"  3. Test on full test set\")\n",
    "print(f\"  4. Compare with baseline models\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Test completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results for later analysis\n",
    "print(\"\\n💾 Saving results for analysis...\")\n",
    "\n",
    "# Create results dictionary for export\n",
    "export_results = {\n",
    "    'test_info': {\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'num_examples_tested': num_examples,\n",
    "        'chunking_config': {\n",
    "            'max_tokens': 1024,\n",
    "            'overlap_tokens': 128,\n",
    "            'use_sentence_boundaries': True\n",
    "        },\n",
    "        'summarization_config': {\n",
    "            'chunk_max_length': config.chunk_max_length,\n",
    "            'cluster_max_length': config.cluster_max_length,\n",
    "            'fusion_strategy': config.fusion_strategy\n",
    "        }\n",
    "    },\n",
    "    'single_example_test': {\n",
    "        'chunks': chunks,\n",
    "        'result': result,\n",
    "        'chunk_stats': chunk_stats\n",
    "    },\n",
    "    'fusion_comparison': fusion_results,\n",
    "    'batch_results': batch_results,\n",
    "    'statistics': {\n",
    "        'avg_compression': float(batch_df['compression_ratio'].mean()),\n",
    "        'avg_output_tokens': float(batch_df['output_tokens'].mean()),\n",
    "        'avg_chunks': float(batch_df['num_chunks'].mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Results saved to 'export_results' variable\")\n",
    "print(\"\\nYou can access:\")\n",
    "print(\"  • export_results['single_example_test'] - Detailed first example\")\n",
    "print(\"  • export_results['batch_results'] - All batch processing results\")\n",
    "print(\"  • export_results['fusion_comparison'] - Strategy comparison\")\n",
    "print(\"  • batch_df - Pandas DataFrame with batch statistics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready for next stage: Fact Verification Module!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ROUGE Evaluation for FactSum Hierarchical Summarization\n",
    "========================================================\n",
    "Comprehensive evaluation metrics including ROUGE-1, ROUGE-2, ROUGE-L,\n",
    "and additional quality metrics for publication-ready results.\n",
    "\n",
    "Install required packages first:\n",
    "    pip install rouge-score nltk bert-score\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "class FactSumEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation suite for FactSum hierarchical summarization.\n",
    "    \n",
    "    Metrics:\n",
    "    - ROUGE-1, ROUGE-2, ROUGE-L (standard summarization metrics)\n",
    "    - Length statistics\n",
    "    - Compression ratios\n",
    "    - Statistical significance testing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_stemmer: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize evaluator.\n",
    "        \n",
    "        Args:\n",
    "            use_stemmer: Whether to use stemming in ROUGE calculation\n",
    "        \"\"\"\n",
    "        self.scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL'], \n",
    "            use_stemmer=use_stemmer\n",
    "        )\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "    def evaluate_single(self, \n",
    "                       generated_summary: str, \n",
    "                       reference_summary: str,\n",
    "                       example_id: int = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a single generated summary against reference.\n",
    "        \n",
    "        Args:\n",
    "            generated_summary: Model-generated summary\n",
    "            reference_summary: Human-written reference summary\n",
    "            example_id: Optional identifier for this example\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with ROUGE scores and statistics\n",
    "        \"\"\"\n",
    "        # Calculate ROUGE scores\n",
    "        scores = self.scorer.score(reference_summary, generated_summary)\n",
    "        \n",
    "        # Extract F1 scores (most commonly reported)\n",
    "        rouge_1_f1 = scores['rouge1'].fmeasure\n",
    "        rouge_2_f1 = scores['rouge2'].fmeasure\n",
    "        rouge_l_f1 = scores['rougeL'].fmeasure\n",
    "        \n",
    "        # Also get precision and recall for detailed analysis\n",
    "        rouge_1_p = scores['rouge1'].precision\n",
    "        rouge_1_r = scores['rouge1'].recall\n",
    "        rouge_2_p = scores['rouge2'].precision\n",
    "        rouge_2_r = scores['rouge2'].recall\n",
    "        rouge_l_p = scores['rougeL'].precision\n",
    "        rouge_l_r = scores['rougeL'].recall\n",
    "        \n",
    "        # Length statistics\n",
    "        gen_length = len(generated_summary.split())\n",
    "        ref_length = len(reference_summary.split())\n",
    "        length_ratio = gen_length / ref_length if ref_length > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            'example_id': example_id,\n",
    "            'rouge1_f1': rouge_1_f1,\n",
    "            'rouge1_precision': rouge_1_p,\n",
    "            'rouge1_recall': rouge_1_r,\n",
    "            'rouge2_f1': rouge_2_f1,\n",
    "            'rouge2_precision': rouge_2_p,\n",
    "            'rouge2_recall': rouge_2_r,\n",
    "            'rougeL_f1': rouge_l_f1,\n",
    "            'rougeL_precision': rouge_l_p,\n",
    "            'rougeL_recall': rouge_l_r,\n",
    "            'generated_length': gen_length,\n",
    "            'reference_length': ref_length,\n",
    "            'length_ratio': length_ratio,\n",
    "            'generated_summary': generated_summary,\n",
    "            'reference_summary': reference_summary\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_batch(self, \n",
    "                      generated_summaries: List[str],\n",
    "                      reference_summaries: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluate multiple summaries.\n",
    "        \n",
    "        Args:\n",
    "            generated_summaries: List of model-generated summaries\n",
    "            reference_summaries: List of human-written reference summaries\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with results for each example\n",
    "        \"\"\"\n",
    "        if len(generated_summaries) != len(reference_summaries):\n",
    "            raise ValueError(\"Number of generated and reference summaries must match\")\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        print(f\"Evaluating {len(generated_summaries)} summaries...\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, (gen, ref) in enumerate(zip(generated_summaries, reference_summaries)):\n",
    "            result = self.evaluate_single(gen, ref, example_id=i)\n",
    "            self.results.append(result)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i + 1}/{len(generated_summaries)} examples\")\n",
    "        \n",
    "        print(f\"✓ Evaluation complete\\n\")\n",
    "        \n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def get_aggregate_scores(self, results_df: pd.DataFrame = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate aggregate statistics across all examples.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with evaluation results (uses self.results if None)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with mean and std for each metric\n",
    "        \"\"\"\n",
    "        if results_df is None:\n",
    "            if not self.results:\n",
    "                raise ValueError(\"No results to aggregate. Run evaluate_batch first.\")\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']\n",
    "        \n",
    "        aggregate = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            aggregate[f'{metric}_mean'] = results_df[metric].mean()\n",
    "            aggregate[f'{metric}_std'] = results_df[metric].std()\n",
    "            aggregate[f'{metric}_median'] = results_df[metric].median()\n",
    "            aggregate[f'{metric}_min'] = results_df[metric].min()\n",
    "            aggregate[f'{metric}_max'] = results_df[metric].max()\n",
    "        \n",
    "        # Length statistics\n",
    "        aggregate['avg_generated_length'] = results_df['generated_length'].mean()\n",
    "        aggregate['avg_reference_length'] = results_df['reference_length'].mean()\n",
    "        aggregate['avg_length_ratio'] = results_df['length_ratio'].mean()\n",
    "        \n",
    "        return aggregate\n",
    "    \n",
    "    def print_summary_table(self, results_df: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Print formatted summary table of results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        if results_df is None:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        aggregate = self.get_aggregate_scores(results_df)\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"ROUGE EVALUATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nDataset: {len(results_df)} examples\")\n",
    "        print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Metric':<20} {'Mean':<12} {'Std':<12} {'Median':<12} {'Min':<12} {'Max':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
    "            metric_name = metric.replace('_f1', '').upper()\n",
    "            print(f\"{metric_name:<20} \"\n",
    "                  f\"{aggregate[f'{metric}_mean']:<12.4f} \"\n",
    "                  f\"{aggregate[f'{metric}_std']:<12.4f} \"\n",
    "                  f\"{aggregate[f'{metric}_median']:<12.4f} \"\n",
    "                  f\"{aggregate[f'{metric}_min']:<12.4f} \"\n",
    "                  f\"{aggregate[f'{metric}_max']:<12.4f}\")\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        print(f\"\\nLength Statistics:\")\n",
    "        print(f\"  Average generated length: {aggregate['avg_generated_length']:.1f} words\")\n",
    "        print(f\"  Average reference length: {aggregate['avg_reference_length']:.1f} words\")\n",
    "        print(f\"  Average length ratio: {aggregate['avg_length_ratio']:.2f}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def plot_rouge_distributions(self, results_df: pd.DataFrame = None, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Create visualization of ROUGE score distributions.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with evaluation results\n",
    "            save_path: Optional path to save figure\n",
    "        \"\"\"\n",
    "        if results_df is None:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: ROUGE F1 distributions\n",
    "        ax1 = axes[0, 0]\n",
    "        rouge_metrics = ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']\n",
    "        positions = [1, 2, 3]\n",
    "        \n",
    "        data_to_plot = [results_df[metric].values for metric in rouge_metrics]\n",
    "        bp = ax1.boxplot(data_to_plot, positions=positions, widths=0.6, patch_artist=True)\n",
    "        \n",
    "        for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen', 'lightcoral']):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax1.set_xticklabels(['ROUGE-1', 'ROUGE-2', 'ROUGE-L'])\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('ROUGE Score Distributions')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: ROUGE-1 components (Precision vs Recall)\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.scatter(results_df['rouge1_recall'], results_df['rouge1_precision'], alpha=0.6)\n",
    "        ax2.plot([0, 1], [0, 1], 'r--', alpha=0.3, label='Perfect P=R')\n",
    "        ax2.set_xlabel('ROUGE-1 Recall')\n",
    "        ax2.set_ylabel('ROUGE-1 Precision')\n",
    "        ax2.set_title('ROUGE-1: Precision vs Recall')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: ROUGE scores across examples\n",
    "        ax3 = axes[1, 0]\n",
    "        x = range(len(results_df))\n",
    "        ax3.plot(x, results_df['rouge1_f1'], label='ROUGE-1', alpha=0.7, marker='o', markersize=3)\n",
    "        ax3.plot(x, results_df['rouge2_f1'], label='ROUGE-2', alpha=0.7, marker='s', markersize=3)\n",
    "        ax3.plot(x, results_df['rougeL_f1'], label='ROUGE-L', alpha=0.7, marker='^', markersize=3)\n",
    "        ax3.set_xlabel('Example Index')\n",
    "        ax3.set_ylabel('F1 Score')\n",
    "        ax3.set_title('ROUGE Scores Across Examples')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Length ratio vs ROUGE-1\n",
    "        ax4 = axes[1, 1]\n",
    "        scatter = ax4.scatter(results_df['length_ratio'], results_df['rouge1_f1'], \n",
    "                             c=results_df['rouge2_f1'], cmap='viridis', alpha=0.6)\n",
    "        ax4.axvline(x=1.0, color='r', linestyle='--', alpha=0.3, label='Ideal length ratio')\n",
    "        ax4.set_xlabel('Length Ratio (Generated/Reference)')\n",
    "        ax4.set_ylabel('ROUGE-1 F1')\n",
    "        ax4.set_title('Summary Length vs Quality')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.colorbar(scatter, ax=ax4, label='ROUGE-2 F1')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✓ Figure saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def compare_systems(self, \n",
    "                       system_results: Dict[str, pd.DataFrame],\n",
    "                       save_path: str = None):\n",
    "        \"\"\"\n",
    "        Compare multiple systems side-by-side.\n",
    "        \n",
    "        Args:\n",
    "            system_results: Dict mapping system names to their results DataFrames\n",
    "            save_path: Optional path to save comparison plot\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"SYSTEM COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        comparison_data = []\n",
    "        \n",
    "        for system_name, results_df in system_results.items():\n",
    "            aggregate = self.get_aggregate_scores(results_df)\n",
    "            comparison_data.append({\n",
    "                'System': system_name,\n",
    "                'ROUGE-1': aggregate['rouge1_f1_mean'],\n",
    "                'ROUGE-2': aggregate['rouge2_f1_mean'],\n",
    "                'ROUGE-L': aggregate['rougeL_f1_mean'],\n",
    "                'Avg Length': aggregate['avg_generated_length']\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(\"\\n\", comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        x = np.arange(len(system_results))\n",
    "        width = 0.25\n",
    "        \n",
    "        metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "        colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "        \n",
    "        for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "            values = comparison_df[metric].values\n",
    "            ax.bar(x + i*width, values, width, label=metric, color=color, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('System')\n",
    "        ax.set_ylabel('F1 Score')\n",
    "        ax.set_title('System Comparison: ROUGE Scores')\n",
    "        ax.set_xticks(x + width)\n",
    "        ax.set_xticklabels(comparison_df['System'])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✓ Comparison figure saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def export_results(self, filename: str = 'factsum_rouge_results.csv'):\n",
    "        \"\"\"\n",
    "        Export detailed results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output CSV filename\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            raise ValueError(\"No results to export. Run evaluate_batch first.\")\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"✓ Results exported to {filename}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TESTING CODE FOR JUPYTER NOTEBOOK\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ROUGE Evaluation Testing Code\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nFor Jupyter notebook, use:\\n\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "# =============================================================================\n",
    "# ROUGE EVALUATION - COMPLETE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ROUGE EVALUATION FOR FACTSUM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = FactSumEvaluator(use_stemmer=True)\n",
    "\n",
    "# Extract generated and reference summaries from batch_results\n",
    "generated_summaries = [r['generated_summary'] for r in batch_results]\n",
    "reference_summaries = [r['reference_summary'] for r in batch_results]\n",
    "\n",
    "print(f\"\\\\nEvaluating {len(generated_summaries)} summaries...\\\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "rouge_results = evaluator.evaluate_batch(generated_summaries, reference_summaries)\n",
    "\n",
    "# Print summary table\n",
    "evaluator.print_summary_table(rouge_results)\n",
    "\n",
    "# Visualize results\n",
    "evaluator.plot_rouge_distributions(rouge_results, save_path='rouge_distributions.png')\n",
    "\n",
    "# Show detailed results for first few examples\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS - FIRST 3 EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(rouge_results))):\n",
    "    print(f\"\\\\nExample {i}:\")\n",
    "    print(f\"  ROUGE-1: {rouge_results.iloc[i]['rouge1_f1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_results.iloc[i]['rouge2_f1']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge_results.iloc[i]['rougeL_f1']:.4f}\")\n",
    "    print(f\"  Length ratio: {rouge_results.iloc[i]['length_ratio']:.2f}\")\n",
    "    print(f\"  Generated ({rouge_results.iloc[i]['generated_length']} words):\")\n",
    "    print(f\"    {rouge_results.iloc[i]['generated_summary'][:150]}...\")\n",
    "    print(f\"  Reference ({rouge_results.iloc[i]['reference_length']} words):\")\n",
    "    print(f\"    {rouge_results.iloc[i]['reference_summary'][:150]}...\")\n",
    "\n",
    "# Export results\n",
    "evaluator.export_results('factsum_rouge_results.csv')\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARE FUSION STRATEGIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"COMPARING FUSION STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# You need to re-run summarization with different strategies and store results\n",
    "# Example structure:\n",
    "\n",
    "# Run with concatenate strategy\n",
    "config.fusion_strategy = \"concatenate\"\n",
    "summarizer.reset_statistics()\n",
    "\n",
    "concat_results = []\n",
    "for idx in range(3):\n",
    "    example = multi_news[\"test\"][idx]\n",
    "    chunks = chunker.chunk_document(example[\"document\"])\n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    concat_results.append({\n",
    "        'generated_summary': result['final_summary'],\n",
    "        'reference_summary': example['summary']\n",
    "    })\n",
    "\n",
    "# Evaluate concatenate\n",
    "concat_gen = [r['generated_summary'] for r in concat_results]\n",
    "concat_ref = [r['reference_summary'] for r in concat_results]\n",
    "concat_rouge = evaluator.evaluate_batch(concat_gen, concat_ref)\n",
    "\n",
    "# Run with iterative strategy\n",
    "config.fusion_strategy = \"iterative\"\n",
    "summarizer.reset_statistics()\n",
    "\n",
    "iter_results = []\n",
    "for idx in range(3):\n",
    "    example = multi_news[\"test\"][idx]\n",
    "    chunks = chunker.chunk_document(example[\"document\"])\n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    iter_results.append({\n",
    "        'generated_summary': result['final_summary'],\n",
    "        'reference_summary': example['summary']\n",
    "    })\n",
    "\n",
    "# Evaluate iterative\n",
    "iter_gen = [r['generated_summary'] for r in iter_results]\n",
    "iter_ref = [r['reference_summary'] for r in iter_results]\n",
    "iter_rouge = evaluator.evaluate_batch(iter_gen, iter_ref)\n",
    "\n",
    "# Compare systems\n",
    "system_comparison = evaluator.compare_systems({\n",
    "    'Concatenate': concat_rouge,\n",
    "    'Iterative': iter_rouge\n",
    "}, save_path='fusion_comparison.png')\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"FUSION STRATEGY COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(system_comparison)\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TEST (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Paired t-test between concatenate and iterative\n",
    "for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
    "    t_stat, p_value = stats.ttest_rel(\n",
    "        concat_rouge[metric].values,\n",
    "        iter_rouge[metric].values\n",
    "    )\n",
    "    \n",
    "    metric_name = metric.replace('_f1', '').upper()\n",
    "    print(f\"\\\\n{metric_name}:\")\n",
    "    print(f\"  Concatenate: {concat_rouge[metric].mean():.4f} ± {concat_rouge[metric].std():.4f}\")\n",
    "    print(f\"  Iterative: {iter_rouge[metric].mean():.4f} ± {iter_rouge[metric].std():.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant: {'✓ YES' if p_value < 0.05 else '✗ NO'}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52876d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ROUGE EVALUATION - COMPLETE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ROUGE EVALUATION FOR FACTSUM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = FactSumEvaluator(use_stemmer=True)\n",
    "\n",
    "# Extract generated and reference summaries from batch_results\n",
    "generated_summaries = [r['generated_summary'] for r in batch_results]\n",
    "reference_summaries = [r['reference_summary'] for r in batch_results]\n",
    "\n",
    "print(f\"\\nEvaluating {len(generated_summaries)} summaries...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "rouge_results = evaluator.evaluate_batch(generated_summaries, reference_summaries)\n",
    "\n",
    "# Print summary table\n",
    "evaluator.print_summary_table(rouge_results)\n",
    "\n",
    "# Visualize results\n",
    "evaluator.plot_rouge_distributions(rouge_results, save_path='rouge_distributions.png')\n",
    "\n",
    "# Show detailed results for first few examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED RESULTS - FIRST 3 EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(rouge_results))):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  ROUGE-1: {rouge_results.iloc[i]['rouge1_f1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_results.iloc[i]['rouge2_f1']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge_results.iloc[i]['rougeL_f1']:.4f}\")\n",
    "    print(f\"  Length ratio: {rouge_results.iloc[i]['length_ratio']:.2f}\")\n",
    "    print(f\"  Generated ({rouge_results.iloc[i]['generated_length']} words):\")\n",
    "    print(f\"    {rouge_results.iloc[i]['generated_summary'][:150]}...\")\n",
    "    print(f\"  Reference ({rouge_results.iloc[i]['reference_length']} words):\")\n",
    "    print(f\"    {rouge_results.iloc[i]['reference_summary'][:150]}...\")\n",
    "\n",
    "# Export results\n",
    "evaluator.export_results('factsum_rouge_results.csv')\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARE FUSION STRATEGIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARING FUSION STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# You need to re-run summarization with different strategies and store results\n",
    "# Example structure:\n",
    "\n",
    "# Run with concatenate strategy\n",
    "config.fusion_strategy = \"concatenate\"\n",
    "summarizer.reset_statistics()\n",
    "\n",
    "concat_results = []\n",
    "for idx in range(3):\n",
    "    example = multi_news[\"test\"][idx]\n",
    "    chunks = chunker.chunk_document(example[\"document\"])\n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    concat_results.append({\n",
    "        'generated_summary': result['final_summary'],\n",
    "        'reference_summary': example['summary']\n",
    "    })\n",
    "\n",
    "# Evaluate concatenate\n",
    "concat_gen = [r['generated_summary'] for r in concat_results]\n",
    "concat_ref = [r['reference_summary'] for r in concat_results]\n",
    "concat_rouge = evaluator.evaluate_batch(concat_gen, concat_ref)\n",
    "\n",
    "# Run with iterative strategy\n",
    "config.fusion_strategy = \"iterative\"\n",
    "summarizer.reset_statistics()\n",
    "\n",
    "iter_results = []\n",
    "for idx in range(3):\n",
    "    example = multi_news[\"test\"][idx]\n",
    "    chunks = chunker.chunk_document(example[\"document\"])\n",
    "    result = summarizer.summarize_document(chunks, return_chunk_summaries=False)\n",
    "    iter_results.append({\n",
    "        'generated_summary': result['final_summary'],\n",
    "        'reference_summary': example['summary']\n",
    "    })\n",
    "\n",
    "# Evaluate iterative\n",
    "iter_gen = [r['generated_summary'] for r in iter_results]\n",
    "iter_ref = [r['reference_summary'] for r in iter_results]\n",
    "iter_rouge = evaluator.evaluate_batch(iter_gen, iter_ref)\n",
    "\n",
    "# Compare systems\n",
    "system_comparison = evaluator.compare_systems({\n",
    "    'Concatenate': concat_rouge,\n",
    "    'Iterative': iter_rouge\n",
    "}, save_path='fusion_comparison.png')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FUSION STRATEGY COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(system_comparison)\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TEST (Optional)\n",
    "# =============================================================================\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Paired t-test between concatenate and iterative\n",
    "for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
    "    t_stat, p_value = stats.ttest_rel(\n",
    "        concat_rouge[metric].values,\n",
    "        iter_rouge[metric].values\n",
    "    )\n",
    "    \n",
    "    metric_name = metric.replace('_f1', '').upper()\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Concatenate: {concat_rouge[metric].mean():.4f} ± {concat_rouge[metric].std():.4f}\")\n",
    "    print(f\"  Iterative: {iter_rouge[metric].mean():.4f} ± {iter_rouge[metric].std():.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant: {'✓ YES' if p_value < 0.05 else '✗ NO'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PRIMERA vs PEGASUS Comparison - Using Your Existing Pipeline\n",
    "=============================================================\n",
    "This version uses your EXACT SemanticDocumentChunker and \n",
    "HierarchicalSummarizer classes from earlier.\n",
    "\n",
    "Just runs the same pipeline twice: once with PEGASUS, once with PRIMERA.\n",
    "\n",
    "Prerequisites:\n",
    "- Your existing classes must be loaded in the notebook:\n",
    "  * SemanticDocumentChunker\n",
    "  * SummarizationConfig  \n",
    "  * HierarchicalSummarizer\n",
    "  * FactSumEvaluator\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FAIR MODEL COMPARISON: PRIMERA vs PEGASUS\")\n",
    "print(\"Using YOUR Existing Pipeline Classes\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Check if required classes exist\n",
    "try:\n",
    "    # Test if classes are available\n",
    "    test_chunker = SemanticDocumentChunker\n",
    "    test_config = SummarizationConfig\n",
    "    test_summarizer = HierarchicalSummarizer\n",
    "    test_evaluator = FactSumEvaluator\n",
    "    print(\"✓ All required classes found in notebook\\n\")\n",
    "except NameError as e:\n",
    "    print(f\"❌ ERROR: Required class not found: {e}\")\n",
    "    print(\"\\nPlease make sure you've run the cells containing:\")\n",
    "    print(\"  • SemanticDocumentChunker\")\n",
    "    print(\"  • SummarizationConfig\")\n",
    "    print(\"  • HierarchicalSummarizer\")\n",
    "    print(\"  • FactSumEvaluator\")\n",
    "    raise\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "NUM_EXAMPLES = 10  # Number of examples to test\n",
    "\n",
    "models_to_test = {\n",
    "    'PEGASUS': 'google/pegasus-multi_news',\n",
    "    'PRIMERA': 'allenai/PRIMERA'\n",
    "}\n",
    "\n",
    "# Shared configuration (identical for both models)\n",
    "shared_config = SummarizationConfig(\n",
    "    chunk_max_length=256,\n",
    "    chunk_min_length=64,\n",
    "    chunk_num_beams=4,\n",
    "    cluster_max_length=256,\n",
    "    cluster_min_length=100,\n",
    "    cluster_num_beams=6,\n",
    "    fusion_strategy=\"concatenate\"  # or \"iterative\"\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Number of test examples: {NUM_EXAMPLES}\")\n",
    "print(f\"  Chunk summary length: {shared_config.chunk_min_length}-{shared_config.chunk_max_length}\")\n",
    "print(f\"  Final summary length: {shared_config.cluster_min_length}-{shared_config.cluster_max_length}\")\n",
    "print(f\"  Fusion strategy: {shared_config.fusion_strategy}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# INITIALIZE MODELS AND PIPELINES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING PIPELINES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store chunkers and summarizers for each model\n",
    "chunkers = {}\n",
    "summarizers = {}\n",
    "\n",
    "for model_name, model_id in models_to_test.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Initialize chunker for this model\n",
    "    print(f\"  Initializing chunker...\")\n",
    "    chunkers[model_name] = SemanticDocumentChunker(\n",
    "        model_name=model_id,\n",
    "        max_tokens=1024,\n",
    "        overlap_tokens=128,\n",
    "        use_sentence_boundaries=True,\n",
    "        min_chunk_tokens=256,\n",
    "        preserve_paragraphs=True\n",
    "    )\n",
    "    \n",
    "    # Initialize summarizer for this model\n",
    "    print(f\"  Initializing hierarchical summarizer...\")\n",
    "    summarizers[model_name] = HierarchicalSummarizer(\n",
    "        model_name=model_id,\n",
    "        config=shared_config\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ {model_name} pipeline ready\")\n",
    "\n",
    "print(\"\\n✓ All pipelines initialized\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT 1: SINGLE DOCUMENT DETAILED COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 1: Single Document Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get first test example\n",
    "test_example = multi_news[\"test\"][0]\n",
    "\n",
    "print(f\"\\nTest document:\")\n",
    "print(f\"  Length: {len(test_example['document'])} characters\")\n",
    "print(f\"  Reference summary: {len(test_example['summary'])} characters\")\n",
    "print()\n",
    "\n",
    "single_doc_results = {}\n",
    "\n",
    "for model_name in models_to_test.keys():\n",
    "    print(f\"{model_name} Processing:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Step 1: Chunk the document\n",
    "    print(\"  Stage 1: Semantic chunking...\")\n",
    "    chunks = chunkers[model_name].chunk_document(test_example['document'])\n",
    "    \n",
    "    # Validate chunks\n",
    "    is_valid, warnings_list = chunkers[model_name].validate_chunks(chunks)\n",
    "    print(f\"    ✓ Created {len(chunks)} chunks (valid: {is_valid})\")\n",
    "    \n",
    "    # Get chunking stats\n",
    "    chunk_stats = chunkers[model_name].get_summary_statistics(chunks)\n",
    "    print(f\"    Token efficiency: {chunk_stats['token_efficiency']:.2f}%\")\n",
    "    print(f\"    Avg tokens per chunk: {chunk_stats['avg_tokens_per_chunk']:.1f}\")\n",
    "    \n",
    "    # Step 2: Hierarchical summarization\n",
    "    print(\"  Stage 2: Hierarchical summarization...\")\n",
    "    summarizers[model_name].reset_statistics()\n",
    "    result = summarizers[model_name].summarize_document(chunks, return_chunk_summaries=True)\n",
    "    \n",
    "    print(f\"    ✓ Final summary: {len(result['final_summary'].split())} words\")\n",
    "    print(f\"    Compression: {result['total_compression_ratio']:.2f}x\")\n",
    "    \n",
    "    # Store results\n",
    "    single_doc_results[model_name] = {\n",
    "        'chunks': chunks,\n",
    "        'chunk_stats': chunk_stats,\n",
    "        'result': result,\n",
    "        'num_chunks': len(chunks)\n",
    "    }\n",
    "    print()\n",
    "\n",
    "# Display detailed comparison\n",
    "print(\"=\"*80)\n",
    "print(\"CHUNKING COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, data in single_doc_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Number of chunks: {data['num_chunks']}\")\n",
    "    print(f\"  Token efficiency: {data['chunk_stats']['token_efficiency']:.2f}%\")\n",
    "    print(f\"  Avg overlap: {data['chunk_stats']['avg_overlap_tokens']:.1f} tokens\")\n",
    "    \n",
    "    for i, chunk in enumerate(data['chunks'][:3]):  # Show first 3\n",
    "        print(f\"    Chunk {i}: {chunk['token_count']} tokens, {chunk['sentence_count']} sentences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHUNK-LEVEL SUMMARIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, data in single_doc_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for i, cs in enumerate(data['result']['chunk_summaries'][:2]):  # Show first 2\n",
    "        print(f\"  Chunk {i}: {cs['summary'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nHUMAN REFERENCE:\")\n",
    "print(test_example['summary'])\n",
    "\n",
    "for model_name, data in single_doc_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(data['result']['final_summary'])\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT 2: BATCH EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 2: Batch Evaluation\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProcessing {NUM_EXAMPLES} examples...\\n\")\n",
    "\n",
    "batch_results = {model_name: [] for model_name in models_to_test.keys()}\n",
    "\n",
    "for idx in range(NUM_EXAMPLES):\n",
    "    example = multi_news[\"test\"][idx]\n",
    "    \n",
    "    print(f\"Example {idx + 1}/{NUM_EXAMPLES}:\")\n",
    "    \n",
    "    for model_name in models_to_test.keys():\n",
    "        print(f\"  {model_name}...\", end=\" \")\n",
    "        \n",
    "        try:\n",
    "            # Chunk document\n",
    "            chunks = chunkers[model_name].chunk_document(example['document'])\n",
    "            \n",
    "            # Summarize\n",
    "            summarizers[model_name].reset_statistics()\n",
    "            result = summarizers[model_name].summarize_document(\n",
    "                chunks, \n",
    "                return_chunk_summaries=False\n",
    "            )\n",
    "            \n",
    "            batch_results[model_name].append({\n",
    "                'example_id': idx,\n",
    "                'generated_summary': result['final_summary'],\n",
    "                'reference_summary': example['summary'],\n",
    "                'num_chunks': result['num_chunks'],\n",
    "                'compression_ratio': result['total_compression_ratio'],\n",
    "                'gen_length': len(result['final_summary'].split()),\n",
    "                'ref_length': len(example['summary'].split())\n",
    "            })\n",
    "            \n",
    "            print(f\"✓ ({result['num_chunks']} chunks, {len(result['final_summary'].split())} words)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {str(e)}\")\n",
    "            batch_results[model_name].append({\n",
    "                'example_id': idx,\n",
    "                'generated_summary': \"\",\n",
    "                'reference_summary': example['summary'],\n",
    "                'num_chunks': 0,\n",
    "                'compression_ratio': 0,\n",
    "                'gen_length': 0,\n",
    "                'ref_length': len(example['summary'].split())\n",
    "            })\n",
    "\n",
    "print(f\"\\n✓ Batch processing complete\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT 3: ROUGE EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 3: ROUGE Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = FactSumEvaluator(use_stemmer=True)\n",
    "\n",
    "rouge_results = {}\n",
    "\n",
    "for model_name in models_to_test.keys():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Extract summaries\n",
    "    generated = [r['generated_summary'] for r in batch_results[model_name] if r['generated_summary']]\n",
    "    reference = [r['reference_summary'] for r in batch_results[model_name] if r['generated_summary']]\n",
    "    \n",
    "    # Evaluate\n",
    "    results_df = evaluator.evaluate_batch(generated, reference)\n",
    "    \n",
    "    # Add metadata\n",
    "    for i, row in results_df.iterrows():\n",
    "        results_df.at[i, 'num_chunks'] = batch_results[model_name][i]['num_chunks']\n",
    "        results_df.at[i, 'compression_ratio'] = batch_results[model_name][i]['compression_ratio']\n",
    "    \n",
    "    rouge_results[model_name] = results_df\n",
    "    \n",
    "    print(f\"  ✓ Evaluated {len(results_df)} examples\")\n",
    "\n",
    "# Display aggregate scores\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ROUGE SCORES COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models_to_test.keys():\n",
    "    df = rouge_results[model_name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'ROUGE-1': f\"{df['rouge1_f1'].mean():.4f} ± {df['rouge1_f1'].std():.4f}\",\n",
    "        'ROUGE-2': f\"{df['rouge2_f1'].mean():.4f} ± {df['rouge2_f1'].std():.4f}\",\n",
    "        'ROUGE-L': f\"{df['rougeL_f1'].mean():.4f} ± {df['rougeL_f1'].std():.4f}\",\n",
    "        'Avg Chunks': f\"{df['num_chunks'].mean():.1f}\",\n",
    "        'Avg Length': f\"{df['generated_length'].mean():.1f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT 4: STATISTICAL SIGNIFICANCE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 4: Statistical Significance Testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pegasus_df = rouge_results['PEGASUS']\n",
    "primera_df = rouge_results['PRIMERA']\n",
    "\n",
    "print(\"\\nPaired t-test (PRIMERA vs PEGASUS):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "significance_results = {}\n",
    "\n",
    "for metric in ['rouge1_f1', 'rouge2_f1', 'rougeL_f1']:\n",
    "    t_stat, p_value = stats.ttest_rel(\n",
    "        pegasus_df[metric].values,\n",
    "        primera_df[metric].values\n",
    "    )\n",
    "    \n",
    "    pegasus_mean = pegasus_df[metric].mean()\n",
    "    primera_mean = primera_df[metric].mean()\n",
    "    improvement = ((primera_mean - pegasus_mean) / pegasus_mean) * 100\n",
    "    \n",
    "    metric_display = metric.replace('_f1', '').replace('rouge', 'ROUGE-')\n",
    "    \n",
    "    print(f\"\\n{metric_display}:\")\n",
    "    print(f\"  PEGASUS:  {pegasus_mean:.4f} ± {pegasus_df[metric].std():.4f}\")\n",
    "    print(f\"  PRIMERA:  {primera_mean:.4f} ± {primera_df[metric].std():.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.2f}%\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant: {'✓ YES' if p_value < 0.05 else '✗ NO'}\")\n",
    "    \n",
    "    significance_results[metric] = {\n",
    "        'pegasus_mean': pegasus_mean,\n",
    "        'primera_mean': primera_mean,\n",
    "        'improvement': improvement,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# EXPERIMENT 5: VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 5: Creating Visualizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "colors = {'PEGASUS': '#3498db', 'PRIMERA': '#e74c3c'}\n",
    "\n",
    "# Plot 1: ROUGE Scores Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "pegasus_scores = [\n",
    "    pegasus_df['rouge1_f1'].mean(),\n",
    "    pegasus_df['rouge2_f1'].mean(),\n",
    "    pegasus_df['rougeL_f1'].mean()\n",
    "]\n",
    "primera_scores = [\n",
    "    primera_df['rouge1_f1'].mean(),\n",
    "    primera_df['rouge2_f1'].mean(),\n",
    "    primera_df['rougeL_f1'].mean()\n",
    "]\n",
    "\n",
    "ax1.bar(x - width/2, pegasus_scores, width, label='PEGASUS', color=colors['PEGASUS'], alpha=0.8)\n",
    "ax1.bar(x + width/2, primera_scores, width, label='PRIMERA', color=colors['PRIMERA'], alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('ROUGE Scores Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: ROUGE-1 Box Plot\n",
    "ax2 = axes[0, 1]\n",
    "bp = ax2.boxplot(\n",
    "    [pegasus_df['rouge1_f1'], primera_df['rouge1_f1']],\n",
    "    labels=['PEGASUS', 'PRIMERA'],\n",
    "    patch_artist=True\n",
    ")\n",
    "for patch, color in zip(bp['boxes'], [colors['PEGASUS'], colors['PRIMERA']]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax2.set_ylabel('ROUGE-1 F1')\n",
    "ax2.set_title('ROUGE-1 Distribution')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Number of Chunks\n",
    "ax3 = axes[0, 2]\n",
    "chunk_means = [pegasus_df['num_chunks'].mean(), primera_df['num_chunks'].mean()]\n",
    "ax3.bar(['PEGASUS', 'PRIMERA'], chunk_means,\n",
    "        color=[colors['PEGASUS'], colors['PRIMERA']], alpha=0.8)\n",
    "ax3.set_ylabel('Average Chunks per Document')\n",
    "ax3.set_title('Chunking Behavior (Should be Similar)')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: ROUGE-1 Across Examples\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(pegasus_df['example_id'], pegasus_df['rouge1_f1'],\n",
    "         marker='o', label='PEGASUS', color=colors['PEGASUS'], alpha=0.7, linewidth=2)\n",
    "ax4.plot(primera_df['example_id'], primera_df['rouge1_f1'],\n",
    "         marker='s', label='PRIMERA', color=colors['PRIMERA'], alpha=0.7, linewidth=2)\n",
    "ax4.set_xlabel('Example ID')\n",
    "ax4.set_ylabel('ROUGE-1 F1')\n",
    "ax4.set_title('ROUGE-1 Per Example')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Summary Length Comparison\n",
    "ax5 = axes[1, 1]\n",
    "ax5.scatter(pegasus_df['reference_length'], pegasus_df['generated_length'],\n",
    "           label='PEGASUS', alpha=0.6, color=colors['PEGASUS'], s=100)\n",
    "ax5.scatter(primera_df['reference_length'], primera_df['generated_length'],\n",
    "           label='PRIMERA', alpha=0.6, color=colors['PRIMERA'], s=100)\n",
    "ax5.plot([0, 200], [0, 200], 'k--', alpha=0.3, label='Perfect match')\n",
    "ax5.set_xlabel('Reference Length (words)')\n",
    "ax5.set_ylabel('Generated Length (words)')\n",
    "ax5.set_title('Summary Length Comparison')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Improvement Percentages\n",
    "ax6 = axes[1, 2]\n",
    "improvements = [\n",
    "    significance_results['rouge1_f1']['improvement'],\n",
    "    significance_results['rouge2_f1']['improvement'],\n",
    "    significance_results['rougeL_f1']['improvement']\n",
    "]\n",
    "significant = [\n",
    "    significance_results['rouge1_f1']['significant'],\n",
    "    significance_results['rouge2_f1']['significant'],\n",
    "    significance_results['rougeL_f1']['significant']\n",
    "]\n",
    "\n",
    "bars = ax6.bar(['ROUGE-1', 'ROUGE-2', 'ROUGE-L'], improvements,\n",
    "               color=['green' if s else 'gray' for s in significant], alpha=0.8)\n",
    "ax6.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "ax6.set_ylabel('Improvement (%)')\n",
    "ax6.set_title('PRIMERA Improvement over PEGASUS')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add significance markers\n",
    "for i, (bar, sig) in enumerate(zip(bars, significant)):\n",
    "    if sig:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                '✓ p<0.05', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison_existing_pipeline.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n✓ Visualization saved as 'model_comparison_existing_pipeline.png'\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL RECOMMENDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rouge1_improvement = significance_results['rouge1_f1']['improvement']\n",
    "rouge1_pval = significance_results['rouge1_f1']['p_value']\n",
    "\n",
    "print(f\"\\nUsing YOUR Existing Hierarchical Pipeline:\")\n",
    "print(f\"  PEGASUS ROUGE-1: {significance_results['rouge1_f1']['pegasus_mean']:.4f}\")\n",
    "print(f\"  PRIMERA ROUGE-1: {significance_results['rouge1_f1']['primera_mean']:.4f}\")\n",
    "print(f\"  Improvement: {rouge1_improvement:+.2f}%\")\n",
    "print(f\"  Statistical significance: p = {rouge1_pval:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  ✓ Both models use YOUR semantic chunking\")\n",
    "print(f\"  ✓ Both models use YOUR hierarchical pipeline\")\n",
    "print(f\"  ✓ Average chunks: {pegasus_df['num_chunks'].mean():.1f} (nearly identical!)\")\n",
    "print(f\"  ✓ Your architectural novelty is fully preserved\")\n",
    "\n",
    "if rouge1_improvement > 3 and rouge1_pval < 0.05:\n",
    "    recommendation = \"PRIMERA\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ RECOMMENDATION: Switch to PRIMERA\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nReasons:\")\n",
    "    print(f\"  • {rouge1_improvement:.1f}% better ROUGE-1 (statistically significant)\")\n",
    "    print(\"  • Works seamlessly with your existing code\")\n",
    "    print(\"  • Just change model_name in one place!\")\n",
    "    print(\"  • All your architectural innovations preserved\")\n",
    "elif rouge1_improvement > 0:\n",
    "    recommendation = \"PRIMERA\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ RECOMMENDATION: Consider PRIMERA\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nReasons:\")\n",
    "    print(f\"  • Marginal improvement ({rouge1_improvement:.1f}%)\")\n",
    "    print(\"  • Not statistically significant yet (p={rouge1_pval:.3f})\")\n",
    "    print(\"  • May become significant with more examples\")\n",
    "    print(\"  • Easy to switch (one line change)\")\n",
    "else:\n",
    "    recommendation = \"PEGASUS\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ RECOMMENDATION: Keep PEGASUS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nReasons:\")\n",
    "    print(\"  • Performance is comparable\")\n",
    "    print(\"  • Already have working pipeline\")\n",
    "    print(\"  • Your innovation is in architecture, not model\")\n",
    "\n",
    "print(f\"\\nImplementation:\")\n",
    "print(f\"  Simply change this in your main code:\")\n",
    "print(f\"  OLD: model_name = 'google/pegasus-multi_news'\")\n",
    "print(f\"  NEW: model_name = 'allenai/PRIMERA'\")\n",
    "print(f\"  Everything else stays EXACTLY the same!\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "export_data = {\n",
    "    'configuration': {\n",
    "        'num_examples': NUM_EXAMPLES,\n",
    "        'chunking': {\n",
    "            'max_tokens': 1024,\n",
    "            'overlap_tokens': 128,\n",
    "            'use_sentence_boundaries': True\n",
    "        },\n",
    "        'summarization': {\n",
    "            'chunk_max_length': shared_config.chunk_max_length,\n",
    "            'cluster_max_length': shared_config.cluster_max_length,\n",
    "            'fusion_strategy': shared_config.fusion_strategy\n",
    "        }\n",
    "    },\n",
    "    'single_document': single_doc_results,\n",
    "    'batch_results': batch_results,\n",
    "    'rouge_results': {k: v.to_dict('records') for k, v in rouge_results.items()},\n",
    "    'comparison_summary': comparison_df.to_dict('records'),\n",
    "    'significance_tests': significance_results,\n",
    "    'recommendation': recommendation\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Results saved to 'export_data' variable\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ COMPARISON COMPLETE!\")\n",
    "print(f\"\\nFinal Answer: Use {recommendation}\")\n",
    "print(\"\\nNext Priority: Implement Fact Verification Module\")\n",
    "print(\"This is where FactSum will truly differentiate from baselines!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a3e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
